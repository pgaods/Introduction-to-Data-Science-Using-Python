{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we study the TensorFlow version of the Embedding() layer as well as word embeddings. We will also introduce anew TensorFlow's API similar to the Keras API. This new API called 'tf.data' is often used conjunction with the Embedding() layers in NLP tasks. We have encountered word embedding problems before using Python libraries such as 'gensim'. Here we will extend the analysis under the TensorFlow framework. \n",
    "\n",
    "This tutorial contains an introduction to word embeddings under the TensorFlow framework. We will train our own word embeddings using a simple Keras model for a sentiment classification task, and then visualize them in the \"Embedding Projector\", a dynamic GUI that can be used to visualize word embeddings within Python script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import io \n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.preprocessing import text_dataset_from_directory  # this only exists beyond TensorFlow 2.5 or above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version:  2.7.0\n",
      "Eager mode:  True\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "#path=\"C:\\\\Users\\\\gao\\\\GAO_Jupyter_Notebook\\\\Datasets\"\n",
    "# path=\"C:\\\\Users\\\\GAO\\\\python workspace\\\\GAO_Jupyter_Notebook\\\\Datasets\"\n",
    "# os.chdir(path)\n",
    "\n",
    "path=\"C:\\\\Users\\\\pgao\\\\Documents\\\\PGZ Documents\\\\Programming Workshop\\\\PYTHON\\\\Open Courses on Python\\\\Udemy Course on Python\\Introduction to Data Science Using Python\\\\datasets\"\n",
    "os.chdir(path)\n",
    "\n",
    "print(\"TensorFlow Version: \", tf.__version__) # this needs version 2.6 or above; if not, in the anaconda prompt, we can run \"pip install --user tensorflow --upgrade\"\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. We do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter we specify). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.\n",
    "\n",
    "In this tutorial we will go over a word-embedding example in TensorFlow. However, before we jump into the actual example, we will need some prerequisite. TensorFlow is a powerful platform where many APIs are available. The one we are familiar with so far is the Keras API. However, when it comes to data science, 'tf.keras' is not the only hot one. Here, we will introduce the 'tf.data' module. This module comes in handy when we deal with large volumes of text data. It is part of the ecosystem of TensorFlow platform that allows more flexible handling of datasets with scaled data preprocessing and transformations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. The 'tf.data.Dataset' API\n",
    "\n",
    "Within the TensorFlow 2.6 version or above, one of the most powerful modules is the 'tf.data' API. Within this module, the 'tf.data.Dataset' is an API that represents a potentially large set of elements, which we will be focusing on in this section. It supports writing descriptive and efficient input pipelines. The 'Dataset' usage follows a common pattern:\n",
    "\n",
    "   - create a source dataset from input data;\n",
    "   - apply dataset transformations to preprocess the data;\n",
    "   - iterate over the dataset and process the elements;\n",
    "   - iteration happens in a streaming fashion, so the full dataset does not need to fit into memory;\n",
    "\n",
    "Let's see some examples of the \"tf.data.Dataset\" API. Below, the simplest way to create a dataset is to create it from a python list. Notice that when we generate the dataset, the type of object is specific to TensorFlow rather than some 'numpy' arrays objects:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>\n",
      "<TensorSliceDataset shapes: (), types: tf.int32>\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "for element in dataset:\n",
    "    print(element)\n",
    "print(type(dataset))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see the results as common 'numpy' type of objects, we can invoke the as_numpy_iterator() method, which returns an iterator converting all elements of the underlying dataset to a 'numpy' type of object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "for element in dataset:\n",
    "    print(element)\n",
    "for element in dataset.as_numpy_iterator():\n",
    "  print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function will also be able to preserve the nested structure of the dataset elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n",
    "                                              'b': [5, 6]})\n",
    "print(type(dataset))\n",
    "list(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n",
    "                                      {'a': (2, 4), 'b': 6}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen a simple example creating an object in the 'Dataset' library. The vision of having this library is to be able to turn different kinds of input into the forms we want for processing. In the example above, we are given a simple tensor to work with. In reality, we may fac different types of data input types or formats. To handle the varieties of data input, the module has many other utilities incoporated. For example, to process lines from files, we can use use tf.data.TextLineDataset():\n",
    "\n",
    "   - dataset = tf.data.TextLineDataset([_file1.txt_, _file2.txt_])\n",
    "\n",
    "To process records written in the 'TFRecord' format, we can use tf.data.TFRecordDataset():\n",
    "\n",
    "   - dataset = tf.data.TFRecordDataset([_file1.tfrecords_, _file2.tfrecords_])\n",
    "\n",
    "To create a dataset of all files matching a pattern, we can use tf.data.Dataset.list_files():\n",
    "\n",
    "   - dataset = tf.data.Dataset.list_files(_/_path/*.txt_)\n",
    "\n",
    "Once we have a dataset, we can apply transformations to prepare the data for your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "dataset = dataset.map(lambda x: x*2)\n",
    "list(dataset.as_numpy_iterator()) # the as_numpy_iterator() method turns the object into a 'numpy' object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the map() method above is an associated function with the specific type of object in the 'Dataset' module. Since the 'dataset' created above from the tensor slice is a class, it must have a set of associated methods. For example, we can use the apply() method to apply a function to the 'Dataset' at hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "def dataset_fn(ds):\n",
    "    return ds.filter(lambda x: x < 5)\n",
    "dataset = dataset.apply(dataset_fn)\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the rest of the section will focus on introducing the most useful methods in the 'tf.data.Dataset' API such as the one above (e.g. the apply() method). Now let's begin our journey and we start with the batch() method which is designed to combine consecutive elements of datasets into batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 1, 2], dtype=int64), array([3, 4, 5], dtype=int64), array([6, 7], dtype=int64)]\n",
      "[array([0, 1, 2], dtype=int64), array([3, 4, 5], dtype=int64), array([6, 7, 8], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(8)\n",
    "dataset = dataset.batch(3)\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "\n",
    "dataset = tf.data.Dataset.range(11)\n",
    "dataset = dataset.batch(3, drop_remainder=True)\n",
    "print(list(dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concatenate() method can concatenate the datasets. Keep in mind that the input dataset and dataset to be concatenated should have compatible element specs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7]\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.ZipDataset'>\n",
      "<ZipDataset shapes: ((), ()), types: (tf.int64, tf.int64)>\n",
      "Two datasets to be concatenated must have the same data type\n"
     ]
    }
   ],
   "source": [
    "a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
    "b = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\n",
    "ds = a.concatenate(b)\n",
    "print(list(ds.as_numpy_iterator()))\n",
    "\n",
    "c = tf.data.Dataset.zip((a, b))\n",
    "print(type(c))\n",
    "print(c)\n",
    "try:\n",
    "    a.concatenate(c)\n",
    "except TypeError:\n",
    "    print('Two datasets to be concatenated must have the same data type')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The enumerate() method can enumerate the elements of datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "(5, 2)\n",
      "(6, 3)\n",
      "------\n",
      "(0, array([7, 8]))\n",
      "(1, array([ 9, 10]))\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "dataset = dataset.enumerate(start=4)\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)\n",
    "\n",
    "print(\"------\")\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)]) # the nested structure of the input dataset determines the structure of elements in the resulting dataset\n",
    "dataset = dataset.enumerate()\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter() method filters the _predicate_; it works like a subset conditionm (i.e. we basically filter on the condition and keep all elements that meet the condition). The method can be used in the following way: \n",
    "\n",
    "   - filter(_predicate_)\n",
    "\n",
    "Here is the example of how we can use this method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "------\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([0, 1, 2, 3])\n",
    "dataset = dataset.filter(lambda x: x < 3)\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "\n",
    "print(\"------\")\n",
    "\n",
    "def filter_fn(x):\n",
    "    return tf.math.equal(x, 1)  # tf.math.equal(x, y) is required for equality comparison\n",
    "dataset = dataset.filter(filter_fn)\n",
    "print(list(dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we have seen the method from_tensor_slices() from examples above. The metho is a useful tool associated with the 'tf.data.Dataset' API. It creates a special type of object called '_tensorflow.python.data.ops.dataset\\_ops.TensorSliceDataset_' whose elements are slices of the given tensors. For simplicity, we will call this type of dataset the 'Dataset' object, as this object is unique in the 'tf.data.Dataset' API. Here is a little more detail on how we can use it. First, the from_tensor_slices() can take different type of inputs: the example below shows that it can take 1D, 2D TENSORS, tuples and dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>\n",
      "[1, 2, 3]\n",
      "[array([1, 2]), array([3, 4])]\n",
      "[(1, 3, 5), (2, 4, 6)]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3]) # slicing a 1D tensor produces scalar tensor elements\n",
    "print(type(dataset))\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]]) # slicing a 2D tensor produces 1D tensor elements\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6])) # slicing a tuple of 1D tensors produces tuple elements containing scalar tensors\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]}) # dictionary structure is also preserved\n",
    "print(list(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3}, {'a': 2, 'b': 4}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also concatenate different tensors together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((2,), ()), types: (tf.int32, tf.string)>\n",
      "(array([1, 3]), b'A')\n",
      "(array([2, 1]), b'B')\n",
      "(array([3, 3]), b'A')\n",
      "-------------------\n",
      "<ZipDataset shapes: ((2,), ()), types: (tf.int32, tf.string)>\n",
      "(array([1, 3]), b'A')\n",
      "(array([2, 1]), b'B')\n",
      "(array([3, 3]), b'A')\n",
      "-------------------\n",
      "(array([[1, 3],\n",
      "       [2, 3]]), array([[b'A'],\n",
      "       [b'A']], dtype=object))\n",
      "(array([[2, 1],\n",
      "       [1, 2]]), array([[b'B'],\n",
      "       [b'B']], dtype=object))\n",
      "(array([[3, 3],\n",
      "       [3, 2]]), array([[b'A'],\n",
      "       [b'B']], dtype=object))\n"
     ]
    }
   ],
   "source": [
    "features = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3-by-2 tensor\n",
    "lbs = tf.constant(['A', 'B', 'A']) # ==> 3-by-1 tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, lbs)) # 2 tensors can be combined into one Dataset object\n",
    "print(dataset)\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)\n",
    "print('--------------')\n",
    "features_dataset = tf.data.Dataset.from_tensor_slices(features) # both the features and the labels tensors can be converted to a 'Dataset' object separately and combined after\n",
    "labels_dataset = tf.data.Dataset.from_tensor_slices(lbs)\n",
    "dataset = tf.data.Dataset.zip((features_dataset, labels_dataset))\n",
    "print(dataset)\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)\n",
    "print('--------------')\n",
    "batched_features = tf.constant([[[1, 3], [2, 3]],\n",
    "                                [[2, 1], [1, 2]],\n",
    "                                [[3, 3], [3, 2]]], shape=(3, 2, 2)) # a batched feature and label set can be converted to a 'Dataset' type of object in similar fashion\n",
    "batched_labels = tf.constant([['A', 'A'],\n",
    "                              ['B', 'B'],\n",
    "                              ['A', 'B']], shape=(3, 2, 1))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((batched_features, batched_labels))\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A corresponding function is from_tensors(). This method creates a 'Dataset' object with a single element, comprising the given tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 2, 3])]\n",
      "[(array([1, 2, 3]), b'A')]\n",
      "[array([1, 2, 3]), array([1, 2, 3])]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensors([1, 2, 3])\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensors(([1, 2, 3], 'A'))\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "\n",
    "example = tf.constant([1,2,3])\n",
    "dataset = tf.data.Dataset.from_tensors(example).repeat(2)\n",
    "print(list(dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep looking at some other important methods. The list_file() method matches file patterns. This is very useful when we want to handle multiple files with similar patterns. For example, if we had the following files on our filesystem:\n",
    "\n",
    "   - /path/to/dir/a.txt\n",
    "   - /path/to/dir/b.py\n",
    "   - /path/to/dir/c.py\n",
    "\n",
    "and if we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:\n",
    "\n",
    "   - /path/to/dir/b.py\n",
    "   - /path/to/dir/c.py\n",
    "\n",
    "This is very similar in SAS and R where we can define a set of files and then let the code read through them iteratively.\n",
    "\n",
    "Another function similar to the R function lapply() is the map(_map\\_func_) method. This method maps its argument '_map\\_func_' across the elements of this dataset. This transformation applies _map\\_func_ to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. Here, _map\\_func_ can be used to change both the values and the structure of a dataset's elements. Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "[2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "ds1 = tf.data.Dataset.range(1, 6)  # [ 1, 2, 3, 4, 5 ]\n",
    "print(list(ds1.as_numpy_iterator()))\n",
    "ds1 = ds1.map(lambda x: x + 1) # the argument takes a single element of type 'tf.Tensor' with the same shape and dtype\n",
    "print(list(ds1.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input signature of _map\\_func_ is determined by the structure of each element in this dataset. Below are some examples to illustrate the varieties of usage. In the first example, each element is a tuple while in the second example. each element is a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[b'Tensor(\"args_0:0\", dtype=int32)foo', b'Tensor(\"args_0:0\", dtype=int32)bar', b'Tensor(\"args_0:0\", dtype=int32)baz']\n"
     ]
    }
   ],
   "source": [
    "elements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")] # each element is a tuple containing two 'tf.Tensor' objects\n",
    "ds2 = tf.data.Dataset.from_generator(lambda: elements, (tf.int32, tf.string))\n",
    "result = ds2.map(lambda x_int, y_str: x_int) # the argument takes 2 arguments of type 'tf.Tensor', and this function projects out just the first component\n",
    "print(list(result.as_numpy_iterator()))\n",
    "\n",
    "elements =  ([{\"a\": 1, \"c\": \"foo\"},\n",
    "              {\"a\": 2, \"c\": \"bar\"},\n",
    "              {\"a\": 3, \"c\": \"baz\"}]) # each element is a dictionary mapping strings to 'tf.Tensor' objects\n",
    "ds3 = tf.data.Dataset.from_generator(lambda: elements, {\"a\": tf.int32, \"c\": tf.string}) \n",
    "result = ds3.map(lambda d: str(d[\"a\"]) + d[\"c\"]) # the argument of map() takes a single argument of type 'dict' with the same keys as the elements\n",
    "print(list(result.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map() method can take on even more complicated functions, some of them can be nested or customized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "TensorSpec(shape=(), dtype=tf.float32, name=None)\n",
      "TensorSpec(shape=(3,), dtype=tf.string, name=None)\n",
      "--------------\n",
      "((TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(2,), dtype=tf.int32, name=None)), TensorSpec(shape=(), dtype=tf.string, name=None))\n",
      "--------------\n",
      "(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(2,), dtype=tf.float64, name=None))\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(3)\n",
    "def g(x):\n",
    "    return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\n",
    "result = dataset.map(g)\n",
    "print(type(result.element_spec)) # this is a tuple\n",
    "for element in result.element_spec:\n",
    "    print(element)\n",
    "print('--------------')\n",
    "\n",
    "dataset = tf.data.Dataset.range(3)\n",
    "def n(x):\n",
    "    return (37.0, [42, 16]), \"foo\"\n",
    "result = dataset.map(n) # the map() method can return nested structures\n",
    "print(result.element_spec) # this is a typle\n",
    "print('--------------')\n",
    "\n",
    "dataset = tf.data.Dataset.range(3)\n",
    "def h(x):\n",
    "  return 37.0, [\"Foo\", \"Bar\"], np.array([1.0, 2.0], dtype=np.float64)\n",
    "result = dataset.map(h) # Python primitives, lists, and numpy arrays are implicitly converted to 'tf.Tensor'\n",
    "print(result.element_spec) # this is a tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map() method is one of the most complicated methods. The online reference has more information. We will not elaborate here.\n",
    "\n",
    "Next, let's examine the padded_batch() method. This function combines consecutive elements of this dataset into padded batches. The transformation combines multiple consecutive elements of the input dataset into a single element. The syntax is:\n",
    "\n",
    "   - padded_batch(_batch\\_size_, _padded\\_shapes_=None, _padding\\_values_=None, _drop\\_remainder_=False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[2 2]\n",
      "[3 3 3]\n",
      "[4 4 4 4]\n",
      "\n",
      "[[1 0]\n",
      " [2 2]]\n",
      "[[3 3 3 0]\n",
      " [4 4 4 4]]\n",
      "\n",
      "[[1 0 0 0 0]\n",
      " [2 2 0 0 0]]\n",
      "[[3 3 3 0 0]\n",
      " [4 4 4 4 0]]\n",
      "\n",
      "[[ 1 -1 -1 -1 -1]\n",
      " [ 2  2 -1 -1 -1]]\n",
      "[[ 3  3  3 -1 -1]\n",
      " [ 4  4  4  4 -1]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A = (tf.data.Dataset.range(1, 5, output_type=tf.int32).map(lambda x: tf.fill([x], x))) # padding to the smallest per-batch size that fits all elements\n",
    "for element in A.as_numpy_iterator():\n",
    "    print(element)\n",
    "print('')\n",
    "\n",
    "B = A.padded_batch(2)\n",
    "for element in B.as_numpy_iterator():\n",
    "    print(element)\n",
    "\n",
    "print('')\n",
    "\n",
    "C = A.padded_batch(2, padded_shapes=5) # padding to a fixed size\n",
    "for element in C.as_numpy_iterator():\n",
    "    print(element)\n",
    "\n",
    "print('')\n",
    "\n",
    "D = A.padded_batch(2, padded_shapes=5, padding_values=-1) # padding with a custom value\n",
    "for element in D.as_numpy_iterator():\n",
    "    print(element)\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some complicated examples involving padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([[ 1,  2,  3, -1],\n",
      "       [ 4,  5, -1, -1]]), array([[ 10, 100],\n",
      "       [ 11,  12]]))]\n",
      "\n",
      "After padding E: \n",
      "(array([[ 1, -1],\n",
      "       [ 2,  2]]), array([[ 1, -1],\n",
      "       [ 2,  2]]))\n",
      "(array([[ 3,  3,  3, -1],\n",
      "       [ 4,  4,  4,  4]]), array([[ 3,  3,  3, -1],\n",
      "       [ 4,  4,  4,  4]]))\n"
     ]
    }
   ],
   "source": [
    "elements = [([1, 2, 3], [10]),\n",
    "            ([4, 5], [11, 12])]\n",
    "dataset0 = tf.data.Dataset.from_generator(lambda: iter(elements), (tf.int32, tf.int32)) # components of nested elements can be padded independently\n",
    "dataset = dataset0.padded_batch(2, padded_shapes=([4], [None]), padding_values=(-1, 100)) # padding the 1st component of the tuple to length 4, and the 2nd component to the smallest size that fits\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "\n",
    "print('')\n",
    "\n",
    "E = tf.data.Dataset.zip((A, A))\n",
    "F = E.padded_batch(2, padding_values=-1) # padding with a single value and multiple components\n",
    "print('After padding E: ')\n",
    "for element in F.as_numpy_iterator():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods so far are fairly complicated conceptually. Let's look at some easier methods. The 'tf.data' module behaves a lot like the 'numpy' package in some way. For example, the random() method creates pseudo-values. We have also seen the range() method that specifies the range of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.TakeDataset'>\n",
      "[3949691112, 1216040066, 825314089, 3986316358, 3335442075, 2799817130, 3237978012, 131803297, 1232301472, 2905322510]\n"
     ]
    }
   ],
   "source": [
    "d1 = tf.data.Dataset.random(seed=4).take(10) # creating a 'Dataset' class with at most 10 elements from the underlying dataset\n",
    "print(type(d1))\n",
    "print(list(d1.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[2, 3, 4]\n",
      "[1, 3]\n",
      "[]\n",
      "[2, 3, 4]\n",
      "[2.0, 3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "print(list(tf.data.Dataset.range(5).as_numpy_iterator()))\n",
    "print(list(tf.data.Dataset.range(2, 5).as_numpy_iterator())) # [2, 3, 4]\n",
    "print(list(tf.data.Dataset.range(1, 5, 2).as_numpy_iterator()))\n",
    "print(list(tf.data.Dataset.range(1, 5, -2).as_numpy_iterator()))\n",
    "print(list(tf.data.Dataset.range(2, 5, output_type=tf.int32).as_numpy_iterator()))\n",
    "print(list(tf.data.Dataset.range(2, 5, output_type=tf.float32).as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduce() method reduces the input dataset to a single element. The syntax for this function is:\n",
    "\n",
    "   - reduce(_initial\\_state_, _reduce\\_func_)\n",
    "\n",
    "The transformation calls _reduce\\_func_ successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The _initial\\_state_ argument is used for the initial state and the final state is returned as the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1).numpy())\n",
    "print(tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The repeat(_c_) (with the argument _c_) method repeats the underlying dataset so each original value is seen _c_ times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 1, 2, 3, 1, 2, 3]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "dataset = dataset.repeat(3) \n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting function within this module is the shard(_num\\_shards_, _index_) method, which is designed to create a 'Dataset' class that includes only 1/_num\\_shards_ of this dataset. Here, the shard is deterministic. The 'Dataset' class produced by $A.shard(n, i)$ will contain all elements of $A$ whose index $mod n = i$ for the dataset $A$. It's a bit hard to describe this function so let's see some examples below. This operator is very useful when running distributed training, as it allows each worker to read a unique subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 6, 9]\n",
      "[1, 4, 7]\n",
      "[2, 5, 8]\n"
     ]
    }
   ],
   "source": [
    "A = tf.data.Dataset.range(10) # [0, 1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "B = A.shard(num_shards=3, index=0) # the original meaning of the word 'shard' means a piece or fragment of a brittle substance (e.g. shards of glass)\n",
    "print(list(B.as_numpy_iterator()))\n",
    "\n",
    "C = A.shard(num_shards=3, index=1)\n",
    "print(list(C.as_numpy_iterator()))\n",
    "\n",
    "D = A.shard(num_shards=3, index=2)\n",
    "print(list(D.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related function is shuffle(_buffer\\_size_, _seed_=None, _reshuffle\\_each\\_iteration_=None), which randomly shuffles the elements of the underlying dataset. This dataset fills a buffer with _buffer\\_size_ elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required. The argument _reshuffle\\_each\\_iteration_ controls whether the shuffle order should be different for each epoch. \n",
    "\n",
    "For instance, if your dataset contains 10000 elements but _buffer\\_size_ is set to 1000, then shuffle will initially select a random element from only the first 1000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1000 element buffer.\n",
    "\n",
    "Below let's see 2 examples for comparison:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 0]\n",
      "[0, 1, 2, 0, 2, 1, 1, 0, 2, 1, 0, 2] \n",
      "\n",
      "[0, 1, 3, 4, 2]\n",
      "[0, 1, 3, 4, 2, 0, 1, 3, 4, 2, 0, 1, 3, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(3) # [0, 1, 2]\n",
    "dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\n",
    "print(list(dataset.as_numpy_iterator())) \n",
    "dataset = dataset.repeat(4)\n",
    "print(list(dataset.as_numpy_iterator()), \"\\n\") \n",
    "\n",
    "dataset = tf.data.Dataset.range(5)\n",
    "dataset = dataset.shuffle(6, reshuffle_each_iteration=False)\n",
    "print(list(dataset.as_numpy_iterator())) \n",
    "dataset = dataset.repeat(3)\n",
    "print(list(dataset.as_numpy_iterator())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's introduce another method: the method skip(_count_) can be useful in that it creates a 'Dataset' class that skips _count_ elements from the underlying dataset. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 8, 9]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.skip(6)\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another good control type of function is take_while(_predicate_), which performs a transformation that stops dataset iteration based on a _predicate_. The _predicate_ cannot just be some random conditions: it has to be a function that maps a nested structure of tensors that satisfy certain criteria. See example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.take_while(lambda x: x < 6)\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unbatch() method is another useful function to unpack datasets. This function splits elements of a dataset into multiple elements. For example, if elements of the dataset are shaped [B, a0, a1, ...], where B may vary for each input element, then for each element in the dataset, the unbatched dataset will contain B consecutive elements of shape [a0, a1, ...]. Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 2, 3], dtype=int64), array([8, 9], dtype=int64), array([ 11,  24, 345,  40], dtype=int64)]\n",
      "[1, 2, 3, 8, 9, 11, 24, 345, 40]\n"
     ]
    }
   ],
   "source": [
    "elements = [ [1, 2, 3], [8, 9], [11, 24, 345, 40] ]\n",
    "dataset = tf.data.Dataset.from_generator(lambda: elements, tf.int64)\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "dataset = dataset.unbatch()\n",
    "print(list(dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove duplicates, we can use the unique() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 37]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 37, 2, 37, 2, 1])\n",
    "dataset = dataset.unique()\n",
    "sorted(list(dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last powerful function in the module is the window() function. This function returns a dataset of \"windows\". Each \"window\" is a dataset that contains a subset of elements of the input dataset. These are finite datasets of size _size_ (or possibly fewer if there are not enough input elements to fill the window and _drop\\_remainder_ evaluates to False). The syntax of the method is the following:\n",
    "\n",
    "   - window(_size_, _shift_=None, _stride_=1, _drop\\_remainder_=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_VariantDataset shapes: (), types: tf.int64>\n",
      "<_VariantDataset shapes: (), types: tf.int64>\n",
      "<_VariantDataset shapes: (), types: tf.int64>\n",
      "-------\n",
      "[0, 1, 2]\n",
      "[3, 4, 5]\n",
      "[6]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(7).window(3)\n",
    "for window in dataset:\n",
    "    print(window)\n",
    "\n",
    "print('-------')\n",
    "\n",
    "for window in dataset:\n",
    "    print([item.numpy() for item in window])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _shift_ argument determines the number of input elements to shift between the start of each window. If windows and elements are both numbered starting at 0, the first element in window $k$ will be element $k \\times _shift_$ of the input dataset. In particular, the first element of the first window will always be the first element of the input dataset. The _stride_ argument determines the stride between input elements within a window. Let's see 2 examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "[1, 2, 3]\n",
      "[2, 3, 4]\n",
      "[3, 4, 5]\n",
      "[4, 5, 6]\n",
      "-----------\n",
      "[0, 2, 4]\n",
      "[1, 3, 5]\n",
      "[2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(7).window(3, shift=1, drop_remainder=True)\n",
    "for window in dataset:\n",
    "    print(list(window.as_numpy_iterator()))\n",
    "\n",
    "print('-----------')\n",
    "\n",
    "dataset = tf.data.Dataset.range(7).window(3, shift=1, stride=2, drop_remainder=True)\n",
    "for window in dataset:\n",
    "    print(list(window.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the window() transformation is applied to a dataset whose elements are nested structures, it produces a dataset where the elements have the same nested structure but each leaf is replaced by a window. In other words, the nesting is applied outside of the windows as opposed inside of them.\n",
    "\n",
    "Applying window() to a 'Dataset' of tuples gives a tuple of windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<_VariantDataset shapes: (), types: tf.int32>,\n",
       " <_VariantDataset shapes: (), types: tf.int32>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3, 4, 5],\n",
    "                                              [6, 7, 8, 9, 10]))\n",
    "dataset = dataset.window(2)\n",
    "windows = next(iter(dataset))\n",
    "windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2] [6, 7]\n",
      "[3, 4] [8, 9]\n",
      "[5] [10]\n"
     ]
    }
   ],
   "source": [
    "def to_numpy(ds):\n",
    "    return list(ds.as_numpy_iterator())\n",
    "\n",
    "for windows in dataset:\n",
    "    print(to_numpy(windows[0]), to_numpy(windows[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying window() to a 'Dataset' of dictionaries gives a dictionary of 'Datasets':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': [1, 2], 'b': [4, 5], 'c': [7, 8]}\n",
      "{'a': [3], 'b': [6], 'c': [9]}\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices({'a': [1, 2, 3],\n",
    "                                              'b': [4, 5, 6],\n",
    "                                              'c': [7, 8, 9]})\n",
    "dataset = dataset.window(2)\n",
    "def to_numpy(ds):\n",
    "    return list(ds.as_numpy_iterator())\n",
    "\n",
    "for windows in dataset:\n",
    "    print(tf.nest.map_structure(to_numpy, windows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the window() function, we often pair the method with the flat_map() method. The flat_map() method can be used to flatten a dataset of windows into a single dataset. The argument to flat_map() is a function that takes an element from the dataset and returns a 'Dataset' class. The method flat_map() chains together the resulting datasets sequentially.\n",
    "\n",
    "For example, to turn each window into a dense tensor, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[1 2 3]\n",
      "[2 3 4]\n",
      "[3 4 5]\n",
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "size = 3\n",
    "dataset = tf.data.Dataset.range(7).window(size, shift=1, drop_remainder=True)\n",
    "batched = dataset.flat_map(lambda x:x.batch(3))\n",
    "for batch in batched:\n",
    "    print(batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. An Illustrative Example of Word Embedding from TensorFlow - Classifiying Movie Reviews\n",
    "\n",
    "Let's use a word embedding example now after studying the 'tf.data.Dataset' API. This example comes from the official TensorFlow documentation. Let's use the famous movie review dataset \"Large Movie Review Dataset\" throughout the tutorial. We will train a sentiment classifier model on this dataset and in the process learn embeddings from scratch. The data contains the text of 50000 movie reviews from the \"Internet Movie Database\". These are split into 25,000 reviews for training and 25,000 reviews for testing. The following codes download all the data and then create a bunch of folder structures in which we will see the training and test folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\aclImdb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['imdb.vocab', 'imdbEr.txt', 'README', 'test', 'train']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "print(dataset_dir)\n",
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'unsup',\n",
       " 'unsupBow.feat',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dir = os.path.join(train_dir, 'unsup') # removing unnecessary stuff\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have created the folder structures, we can take a closer look at the folders. In the 'train' directory, it has 'pos' and 'neg' folders with movie reviews labelled as positive and negative respectively. We will use reviews from both folders to train a binary classification model. The 'train' directory also has additional stuff which should be removed before creating training dataset.\n",
    "\n",
    "The \"aclImdb/train/pos\" and \"aclImdb/train/neg\" directories contain many text files, each of which is a single movie review. Let's take a look at one of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\n"
     ]
    }
   ],
   "source": [
    "sample_file = os.path.join(train_dir, 'pos/1181_9.txt')\n",
    "with open(sample_file) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the \"tf.data.Dataset\" object using tf.keras.preprocessing.text_dataset_from_directory() utility. This method requires a path structure that looks like the following:\n",
    "\n",
    "        main_directory/\n",
    "        ...class_a/\n",
    "        ......a_text_1.txt\n",
    "        ......a_text_2.txt\n",
    "        ......\n",
    "        ...class_b/\n",
    "        ......b_text_1.txt\n",
    "        ......b_text_2.txt\n",
    "        ......\n",
    "\n",
    "If the directory is set up in the way above, then calling tf.keras.preprocessing.text_dataset_from_directory(_main\\_directory_, _labels_='inferred') will return a tf.data.Dataset object that yields batches of texts from the subdirectories _class\\_a_ and _class\\_b_, together with labels 0 and 1 (0 corresponding to _class\\_a_ and 1 corresponding to _class\\_b_):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "seed = 123\n",
    "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size, validation_split=0.2, # 20 percent for validation \n",
    "    subset='training', seed=seed)\n",
    "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size, validation_split=0.2,\n",
    "    subset='validation', seed=seed)\n",
    "\n",
    "print(type(train_ds))\n",
    "print(type(val_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at 3 records (both texts and labels) for the sake of sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 b'Noel Coward,a witty and urbane man,was friends with Louis Mountbatten.Mr Coward,a long-time admirer of all things naval,was commissioned to write a story loosely based on the loss of Mountbatten\\'s ship.In a peculiarly British way it was considered that a film about the Royal Navy losing an encounter at sea would be good propaganda.It was also considered a good idea to have Mr Coward play the part of the ship\\'s captain.Amang the many qualities needed to command a fighting ship,the ability to speak in a very clipped voice and sing sophisticated \"point\" songs does not come very high up the list at Admiralty House,or at least one would hope not.A captain must earn and retain the respect of the wardroom and the lower deck alike. Mr Coward might have had the respect of the gentlemen of the chorus at Drury Lane and Binkie Beaumont might have been terrified of him but his ability to tame,mould and direct a ship\\'s crew in wartime must be brought into question.He folds himself languorously around the bridge,patronising the other ranks and barking orders at the officers,he only needed a silk dressing gown and a cigarette holder to seem right at home. Much is made of the \"warship as a microcosm of British Society\"theme,and the crew largely comprises of the usual cheery cockneys,canny northerners etc.without whom no war can be fought.They spend most of their time on board smoking,moaning about Lord Haw Haw and getting blown up. Never mind,there\\'s plenty more where they came from.Once ashore they go straight to the pub where they spend most of their time smoking,moaning about Lord Haw Haw and getting blown up .By contrast Mr Coward lives in a dream cottage with a rose covered door somewhere very quiet with very little chance of getting blown up.He,his lady wife and their two rosy cheeked cherubs converse in ludicrously convoluted tones and said lady wife spends much of her time knitting things for the poor unfortunates who comprise his crew and who she refers to by their surnames.That nice young master Johnny Mills has a prominent role as a completely unbelievable lower deck type who worships Mr Coward in much the same way as a thrashed dog will worship its master.He marries his girlfriend after kissing her on the cheek,presumably on the grounds that she might be pregnant after such unfettered passion. So yes,we do have a microcosm of British society here,but perhaps not in the way the makers of \"In which we serve\" intended. At the end Mr Coward gets one last chance to patronise his men as the few survivors shuffle past him,\"Goodbye Edwards,it was a privilege to sail with you\"he enunciates as if he was reciting \"How now brown cow\". It may have been David Lean\\'s feature debut,but the hand of Noel Coward looms large right across this picture.He was a funny and clever man,better suited to writing waspish plays about poor little rich girls and boys interspersed with the occasional wry song.He had a talent to amuse,no doubt,but he could neither write nor speak convincing dialogue. Being Noel Coward was a full-time job,he had no time to be a real person.' \n",
      "\n",
      "0 b\"There are pretty landscape shots. Writers putting trite mouthings into actors mouths. With lesser actors this show would be silly. 'Art must uplift humanity or it's BS.' Not so because art of all those mentioned is also to stir humanity and express the dark side. The lead character even says those who don't drink hide the shadow side. Wrong , he lived in darkness and repressed his dark side by drinking and being one dimensional not expanding his horizons with something other than landscapes. There wasn't a breathing organism in his work nor expression of his pain. All the artist did was limit himself to dime a dozen landscapes. The discussions between the characters was grade school, trite stuff always giving the one character the upper hand the writer wanted. I tried to like it after reading all the first wow comments on here. I had to dig deep to see those i agreed with. I figure the great comments were from those connected to the movie. I was moved only once towards the end. The kid was way too passive. The scenery was nice and the music ridiculous. Just my opinion but nowhere show for me.\" \n",
      "\n",
      "0 b'I really tried to like this movie. It deals with an important problem in any society: sex addiction.<br /><br />In this story we learn that you can lose everything when you\\'re addicted to sex. In this case, our main character and hero, for having non-stop sex with all kinds of women (crazy, kinky, neurotic) puts in jeopardy his marriage, job, and even his life.<br /><br />The production values are terrible; mainly the acting. Oh, you won\\'t enjoy ANY of the sex scenes, most of them are done in very poor taste and you might think you\\'re watching a home made flick.<br /><br />Second, the plot is just non sense. How could such a smart and beautiful wife stand all the nasty stuff from the husband? How could she believe him?! The threesome situation is priceless and will make you chuckle for a while.<br /><br />Also, the scene with the black movie theater attendant is just pointless and will leave you thinking \"wtf?\". <br /><br />Scenes like those you will find plenty. <br /><br />Avoid this movie. Please, avoid it; it\\'s not soft core, it\\'s not a documental, it\\'s not a dramatic feature. It\\'s a pretentious effort form a so called documentary director or whatever.<br /><br />Only Mrs. Kinski\\'s legs on display are worth the watch. I caught it on HBO and I\\'m glad I didn\\'t spend my money on it. But those 90 minutes of my life won\\'t come back.' \n",
      "\n",
      "1 b'Paris, je t\\'aime (2006) is a film made up of 18 segments. You can do the math--18 segments in 120 minutes means each director had seven minutes to tell her or his story. The movie is based on the premise that you can, indeed, tell a story in that short amount of time. The premise works. Almost all of the segments are powerful, complete, and satisfying. Each presents a different aspect of the Parisian experience, and almost every director draws forth outstanding performances from a cast of great and near-great actors.<br /><br />There were so many powerful portrayals in this film that it\\'s hard to pick one or two favorites. Probably the most memorable to me were Juliette Binoche as a grieving mother in the segment \"Place des Victoires,\" Gena Rowlands as an aging beauty in \"Quartier Latin,\" Catalina Sandino Moreno as a maid in the segment \"Loin du 16\\xc3\\xa8me\" and Margo Martindale as a Colorado mail carrier who has learned to speak French so she can visit Paris (\"14\\xc3\\xa8me Arrondissement\" segment). <br /><br />Special mention must be given to Gulliver Hecq, probably the meanest little boy to ever harass an American tourist in a Parisian Metro Station (segment \"Tuileries\").<br /><br />This is an outstanding movie. My wife and I decided to rent it in a few months so we can catch some of the subtle points we surely missed. However, Paris is photographed so beautifully that I would suggest that you try to see it on a large screen. In any case, don\\'t miss it!' \n",
      "\n",
      "1 b'This is my favorite classic. It was filmed a little west of Philadelphia, PA when I was 13, in 1957, and released the next year. Then in 1970, I found myself working the very same county as a rookie PA state trooper. I have always enjoyed checking out the different places where scenes were filmed. I knew the owner of the Downingtown Diner well, and he had a road sign out front which told all passing motorists that this was the \"home of the blob\". The theater scene was in Phoenixville, near Valley Forge Park and it is still showing films today!' \n",
      "\n",
      "1 b'I have to say that Higher Learning is one of the top 3 movies I have ever watched. It has a brilliant cast, and an equally brilliant director. Singleton shows how life in University can be. There are 3 main story lines, the skinheads, the African-Americans, and the homosexuals. I was intrigued by all of the stories, but the one that got to me the most was the storyline about Kristen, battling her feelings towards another girl. The end was great. After seeing the movie 25 times plus, I still cry. I would have given this movie an 11, but I have to settlefor 10/10.' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(2): # creating a 'Dataset' object with at most 2 elements from the dataset\n",
    "  for i in range(3):\n",
    "    print(label_batch[i].numpy(), text_batch.numpy()[i], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now before we proceed to the next step of ML, let's configure the dataset for performance optimization, since this is a large dataset which is very common in NLP tasks. These are two important methods we can use when loading data to make sure that I/O does not become blocking:\n",
    "\n",
    "   - The method cache() keeps data in memory after it's loaded off disk. This will ensure the dataset does not become a bottleneck while training our model. If our dataset is too large to fit into memory, we can also use this method to create a performance on-disk cache, which is more efficient to read than many small files.\n",
    "   - The method prefetch() overlaps data preprocessing and model execution while training.  The tf.data.Dataset.prefetch() transformation can be used to decouple the time when data is produced from the time when data is consumed. In particular, the transformation uses a background thread and an internal buffer to 'prefetch' elements from the input dataset ahead of the time they are requested. The number of elements to prefetch should be equal to (or possibly greater than) the number of batches consumed by a single training step. We could either manually tune this value, or set it to 'tf.data.AUTOTUNE', which will prompt the 'tf.data' runtime to tune the value dynamically at runtime.\n",
    "\n",
    "The data performance guide https://www.tensorflow.org/guide/data_performance has more information related to these methods. Here we will do auto-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define the dataset preprocessing steps required for our sentiment classification model. We can initialize a TextVectorization() layer with the desired parameters to vectorize movie reviews. Recall that text data standardization refers to preprocessing the text, typically to remove punctuation or HTML elements to simplify the dataset. Tokenization refers to splitting strings into tokens (for example, splitting a sentence into individual words, by splitting on whitespace). Vectorization refers to converting tokens into numbers so they can be fed into a neural network. All of these tasks can be accomplished with this layer.\n",
    "\n",
    "As we shall see below, the reviews contain various HTML tags like. These tags will not be removed by the default standardizer in the TextVectorization() layer (which converts text to lowercase and strips punctuation by default, but doesn't strip HTML). We thus write a custom standardization function to remove the HTML. The syntax for the method is the following:\n",
    "\n",
    "   - tf.keras.layers.TextVectorization(_max\\_tokens_=None, _standardize_='lower\\_and\\_strip\\_punctuation', _split_='whitespace', _ngrams_=None, _output\\_mode_='int', _output\\_sequence\\_length_=None, _pad\\_to\\_max\\_tokens_=False, _vocabulary_=None, \\*\\*_kwargs_)\n",
    "\n",
    "This tf.keras.layers.TextVectorization() layer has basic options for managing texts in a Keras model. It transforms a batch of strings (i.e., one example = one string) into either a list of token indices (i.e., one example = 1D tensor of integer token indices) or a dense representation (i.e., one example = 1D tensor of float values representing data about the example's tokens).\n",
    "\n",
    "If desired, the user can call this layer's adapt() method on a dataset. When this layer is adapted, it will analyze the dataset, determine the frequency of individual string values, and create a 'vocabulary' from them. This vocabulary can have unlimited size or be capped, depending on the configuration options for this layer; if there are more unique values in the input than the maximum vocabulary size, the most frequent terms will be used to create the vocabulary. A good processing of each example contains the following steps:\n",
    "  \n",
    "   - standardize each example (usually lowercasing + punctuation stripping);\n",
    "   - split each example into substrings (usually words);\n",
    "   - recombine substrings into tokens (usually ngrams);\n",
    "   - index tokens (associate a unique int value with each token);\n",
    "   - transform each example using this index, either into a vector of integers or a dense float vector.\n",
    "\n",
    "The adapt() method fits the state of the preprocessing layer to the data being passed. After calling adapt() on a layer, a preprocessing layer's state will not update during training. More details of this topic can be found here:\n",
    "\n",
    "   - https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization. \n",
    "\n",
    "Let's see an example for illustrative purpose before we continue with our example. Let's suppose we have a toy data derived from a list of strings [\"foo\", \"bar\", \"baz\"]. Let's suppose we want to create a TextVectorization() layer and then build a model for prediction. Here is how the layer works:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before adapt() method applied: ['', '[UNK]']\n",
      "after adapt() method applied: ['', '[UNK]', 'foo', 'devil', 'baz', 'bar']\n",
      "Vocabulary size for the example (after adapt() is applied): 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 5, 0, 0, 0, 0],\n",
       "       [1, 4, 1, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = tf.data.Dataset.from_tensor_slices([\"foo\", \"bar\", \"baz\", \"devil\"])\n",
    "max_features = 500  # maximum vocab size\n",
    "max_len = 7  # sequence length to pad the outputs to (if the number is large then there will be a lot of zero-paddings)\n",
    "\n",
    "vl = tf.keras.layers.TextVectorization(max_tokens=max_features, output_mode='int', output_sequence_length=max_len) # creating the TextVectorization() layer\n",
    "print('before adapt() method applied:', vl.get_vocabulary())\n",
    "vl.adapt(example.batch(64)) # calling adapt() on the text-only dataset to create the vocabulary (no need to batch, but for large datasets this means we are not keeping spare copies of the dataset)\n",
    "print('after adapt() method applied:', vl.get_vocabulary())\n",
    "\n",
    "model_e = tf.keras.models.Sequential() # creating the model that uses the vectorization text layer\n",
    "\n",
    "model_e.add(tf.keras.Input(shape=(1,), dtype=tf.string)) # starting by creating an explicit input layer with a shape of (1,), as we need to guarantee there is exactly one string input per batch.\n",
    "model_e.add(vl) # the  first layer in our model is the vectorization layer, and after this we have a tensor of shape (batch_size, max_len) containing vocab indices\n",
    "print('Vocabulary size for the example (after adapt() is applied): {}'.format(len(vl.get_vocabulary()))) # 6\n",
    "\n",
    "test_input = [[\"foo qux bar\"], [\"qux baz haha wth\"]]\n",
    "model_e.predict(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go back to our original example. Below, we use the text vectorization layer to normalize, split, and map strings to integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data): # creating a custom standardization function to strip HTML break tags '<br />'\n",
    "   lowercase = tf.strings.lower(input_data)\n",
    "   stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "   return tf.strings.regex_replace(stripped_html, '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "vocab_size = 10000 # vocabulary size (number of words in a sequence)\n",
    "sequence_length = 100 # sequence length to pad the outputs to (if the number is large then there will be a lot of zero-paddings)\n",
    "\n",
    "vectorize_layer = TextVectorization( \n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size, # setting maximum_sequence length as all samples are not of the same length\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "text_ds = train_ds.map(lambda x, y: x) # making a text-only dataset (no labels) and call adapt() to build the vocabulary\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287 --->  fully\n",
      " 313 --->  idea\n",
      "Vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1287])\n",
    "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
    "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the Keras Sequential() API to define the sentiment classification model. In this case it is a \"Continuous bag of words\" style model:\n",
    "\n",
    "   - The TextVectorization() layer transforms strings into vocabulary indices. We have already initialized 'vectorize_layer' as a TextVectorization() layer and built it's vocabulary by calling adapt() on the training dataset (here, 'text_ds'). Now the 'vectorize_layer' can be used as the first layer of our end-to-end classification model, feeding transformed strings into the Embedding() layer later.\n",
    "   - We will also need an embedding layer. The Embedding() layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (batch, sequence, embedding).\n",
    "   - The GlobalAveragePooling1D() layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\n",
    "   - The fixed-length output vector is piped through a fully-connected (dense) layer with 16 hidden units.\n",
    "   - The last layer is densely connected with a single output node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization_18 (TextV (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim=16\n",
    "epochs=15\n",
    "\n",
    "model = Sequential([vectorize_layer, Embedding(vocab_size, embedding_dim, name=\"embedding\"), GlobalAveragePooling1D(), Dense(16, activation='relu'), Dense(1)])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's break down what each layer is actually doing. We have discussed the TextVectorization() layer, so now let's talk about the embedding layer. Recall that word embeddings can be thought of as an alternate to one-hot encoding along with dimensionality reduction. The Embedding() layer turns positive integers (indices) into dense vectors of fixed size. The Embedding() layer is basically a matrix which can be considered a transformation from our discrete and sparse 1-hot-vector into a continuous and dense latent space. The embedding layers enable us to convert each word into a fixed length vector of defined size. The resultant vector is a dense one with having real values instead of just 0’s and 1’s. The fixed length of word vectors helps us to represent words in a better way along with reduced dimensions. This way embedding layer works like a lookup table. The words are the keys in this table, while the dense word vectors are the values. \n",
    "\n",
    "The input shape of the Embedding() layer must be a 2D tensor with shape: (_batch\\_size_, _input\\_length_); the output shape of the Embedding() layer must be 3D tensor with shape: (_batch\\_size_, _input\\_length_, _output\\_dim_). Below is an example. The model will take as input an integer matrix of size (_batch_, _input\\_length_) and the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size). The input dimension must be the size of the vocabulary, i.e. maximum integer index + 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 64)\n"
     ]
    }
   ],
   "source": [
    "my_vocab_size=1000 # 1000 as the vocab size\n",
    "my_output_dim=64 # the output dimension is 64 (length of vector for each word)\n",
    "my_input_length=10  # maximum length of sequence is 10\n",
    "\n",
    "mymodel = tf.keras.Sequential()\n",
    "mymodel.add(tf.keras.layers.Embedding(input_dim=my_vocab_size, output_dim=my_output_dim, input_length=my_input_length))\n",
    "input_array = np.random.randint(1000, size=(32, 10)) # input_array.shape=(32, 10)\n",
    "mymodel.compile('rmsprop', 'mse')\n",
    "output_array = mymodel.predict(input_array)\n",
    "print(output_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the output dimension is (32, 10, 64). Let's see another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.02265354  0.02642283 -0.00436708  0.03692282]\n",
      "  [ 0.02681229 -0.04963363  0.00799357 -0.00078764]]]\n"
     ]
    }
   ],
   "source": [
    "mymodel2 = tf.keras.Sequential()\n",
    "embedding_layer2 = tf.keras.layers.Embedding(input_dim=10,output_dim=4,input_length=2)\n",
    "mymodel2.add(embedding_layer2)\n",
    "mymodel2.compile('adam','mse')\n",
    "\n",
    "input_array2 =  np.array([[1,2]])\n",
    "mymodel2.compile('rmsprop', 'mse')\n",
    "output_array2 = mymodel2.predict(input_array2)\n",
    "print(output_array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from the above example, each word (1 and 2) is represented by a vector of length 4. If we print the weights of the embedding layer, we get results below. These weights are basically the vector representations of the words in vocabulary. As we discussed earlier, this is a lookup table of size $10 \\times 4$, for words 0 to 9. The first word (0) is represented by first row in this table. Note that in this example we have not trained the embedding layer. The weights assigned to the word vectors are initialized randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04000392, -0.01862346,  0.04262091,  0.04245141],\n",
       "       [-0.02265354,  0.02642283, -0.00436708,  0.03692282],\n",
       "       [ 0.02681229, -0.04963363,  0.00799357, -0.00078764],\n",
       "       [-0.00102047, -0.01890696,  0.04803513,  0.04678296],\n",
       "       [ 0.0353524 ,  0.00535844,  0.02512223,  0.00242698],\n",
       "       [-0.03001713, -0.04524499,  0.01682706, -0.028548  ],\n",
       "       [ 0.02637163, -0.02148254,  0.02442702, -0.0275311 ],\n",
       "       [ 0.01020578,  0.04947097,  0.0418673 ,  0.02208019],\n",
       "       [ 0.01632892,  0.01952398,  0.02846341,  0.02349358],\n",
       "       [ 0.01891663,  0.00588686,  0.03651556,  0.03078783]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer2.get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the Embedding() layer, we also see a class called GlobalAveragePooling1D(). This class performs the global average pooling operation for temporal data. For this example, the GlobalAveragePooling1D() layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible. \n",
    "\n",
    "The input of the GlobalAveragePooling1D() class must be a 3D tensor, and the output shape depends on the argument. The syntax is:\n",
    "\n",
    "   - tf.keras.layers.GlobalAveragePooling1D(_data\\_format_='channels_last', _keepdims_=False, ** _kwargs_)\n",
    "\n",
    "Specifically:\n",
    "\n",
    "   -The argument _data\\_format_ is one of 'channels\\_last' (default) or 'channels\\_first'. This really just dictates the ordering of the dimensions in the inputs. The 'channels\\_last' option corresponds to inputs with shape (_batch_, _steps_, _features_) while 'channels\\_first' corresponds to inputs with shape (_batch_, _features_, _steps_). Moreover, the argument _keepdim_ has a behavior that is the same as for tf.reduce_mean() or np.mean().\n",
    "\n",
    "   - if _keepdims_=False: then the output would be a 2D tensor with shape (_batchsize_, _features_);\n",
    "   - If _keepdims_=True and if _data\\_format_='channels\\_last': the output would be a 3D tensor with shape (_batchsize_, 1, _features_);\n",
    "   - if _keepdims_=True and if _data\\_format_='channels\\_first': the output would be 3D tensor with shape (_batchsize_, _features_, 1).\n",
    "\n",
    "Let's see an example of using the GlobalAveragePooling1D() layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tf.Tensor(\n",
      "[[[-0.18030666 -0.95028627 -0.03964049 -0.7425406 ]\n",
      "  [ 1.3231523  -0.61854804  0.8540664  -0.08899953]\n",
      "  [ 2.4488697   0.762508    1.2659615   0.9801489 ]]\n",
      "\n",
      " [[ 1.5293121  -0.57500345  0.8987044  -1.250801  ]\n",
      "  [-0.8604956   1.260746   -0.6830498   0.02615766]\n",
      "  [ 0.22328745  0.95914024 -0.37048063  0.03484769]]], shape=(2, 3, 4), dtype=float32) \n",
      "\n",
      "y:\n",
      " tf.Tensor(\n",
      "[[ 1.1972384  -0.26877543  0.69346243  0.04953627]\n",
      " [ 0.297368    0.54829425 -0.05160867 -0.39659855]], shape=(2, 4), dtype=float32) \n",
      "\n",
      "shape of y: (2, 4)\n"
     ]
    }
   ],
   "source": [
    "input_shape = (2, 3, 4)\n",
    "tf.random.set_seed(5) # 2 matrices, each 3-by-4\n",
    "x = tf.random.normal(shape=input_shape, mean=0, stddev=1) \n",
    "print('x:\\n', x, '\\n')\n",
    "y = tf.keras.layers.GlobalAveragePooling1D()(x) # the input must be a 3D tensor\n",
    "print('y:\\n', y, '\\n') # (-0.18030666+1.3231523+2.4488697)/3=1.1972384 etc.\n",
    "print('shape of y:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this is different from the Flatten() layer, which simply flattens the input without affecting the batch size. Here is an example of Flatten() for the sake of comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 10, 64)\n",
      "(None, 640)\n"
     ]
    }
   ],
   "source": [
    "mymodel3 = tf.keras.Sequential()\n",
    "mymodel3.add(tf.keras.layers.Conv2D(64, 3, 3, input_shape=(3, 32, 32)))\n",
    "print(mymodel3.output_shape)\n",
    "\n",
    "mymodel3.add(tf.keras.layers.Flatten())\n",
    "print(mymodel3.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are obviously layers related to GlobalAveragePooling1D(). For example, rather than taking the average, we can take the maximum, hence GlobalMaxPooling1D(). This class downsamples the input representation by taking the maximum value over the time dimension. Here is an illustrative example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]], shape=(3, 3), dtype=float32) \n",
      "\n",
      "original shape of x: (3, 3)\n",
      "tf.Tensor(\n",
      "[[[1.]\n",
      "  [2.]\n",
      "  [3.]]\n",
      "\n",
      " [[4.]\n",
      "  [5.]\n",
      "  [6.]]\n",
      "\n",
      " [[7.]\n",
      "  [8.]\n",
      "  [9.]]], shape=(3, 3, 1), dtype=float32) \n",
      "\n",
      "shape of x after reshaping: (3, 3, 1) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       "array([[3.],\n",
       "       [6.],\n",
       "       [9.]], dtype=float32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]])\n",
    "print(x, \"\\n\")\n",
    "print('original shape of x:', x.shape)\n",
    "x = tf.reshape(x, [3, 3, 1])\n",
    "print(x, \"\\n\")\n",
    "print('shape of x after reshaping:', x.shape, \"\\n\")\n",
    "\n",
    "my_globalmaxpool_1d = tf.keras.layers.GlobalMaxPooling1D()\n",
    "my_globalmaxpool_1d (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done enough illustrative examples to break down the code. Let'S now get back to our main example and train the model on the movie review dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "20/20 [==============================] - 8s 357ms/step - loss: 0.6918 - accuracy: 0.5028 - val_loss: 0.6900 - val_accuracy: 0.4886\n",
      "Epoch 2/15\n",
      "20/20 [==============================] - 3s 155ms/step - loss: 0.6867 - accuracy: 0.5028 - val_loss: 0.6833 - val_accuracy: 0.4886\n",
      "Epoch 3/15\n",
      "20/20 [==============================] - 3s 155ms/step - loss: 0.6773 - accuracy: 0.5028 - val_loss: 0.6718 - val_accuracy: 0.4886\n",
      "Epoch 4/15\n",
      "20/20 [==============================] - 3s 156ms/step - loss: 0.6622 - accuracy: 0.5028 - val_loss: 0.6547 - val_accuracy: 0.4886\n",
      "Epoch 5/15\n",
      "20/20 [==============================] - 3s 155ms/step - loss: 0.6405 - accuracy: 0.5028 - val_loss: 0.6317 - val_accuracy: 0.4886\n",
      "Epoch 6/15\n",
      "20/20 [==============================] - 3s 156ms/step - loss: 0.6127 - accuracy: 0.5199 - val_loss: 0.6039 - val_accuracy: 0.5420\n",
      "Epoch 7/15\n",
      "20/20 [==============================] - 3s 156ms/step - loss: 0.5800 - accuracy: 0.6080 - val_loss: 0.5734 - val_accuracy: 0.6130\n",
      "Epoch 8/15\n",
      "20/20 [==============================] - 3s 157ms/step - loss: 0.5445 - accuracy: 0.6823 - val_loss: 0.5424 - val_accuracy: 0.6662\n",
      "Epoch 9/15\n",
      "20/20 [==============================] - 3s 155ms/step - loss: 0.5088 - accuracy: 0.7369 - val_loss: 0.5132 - val_accuracy: 0.7016\n",
      "Epoch 10/15\n",
      "20/20 [==============================] - 3s 155ms/step - loss: 0.4750 - accuracy: 0.7702 - val_loss: 0.4873 - val_accuracy: 0.7316\n",
      "Epoch 11/15\n",
      "20/20 [==============================] - 3s 155ms/step - loss: 0.4445 - accuracy: 0.7936 - val_loss: 0.4654 - val_accuracy: 0.7496\n",
      "Epoch 12/15\n",
      "20/20 [==============================] - 3s 155ms/step - loss: 0.4176 - accuracy: 0.8115 - val_loss: 0.4473 - val_accuracy: 0.7610\n",
      "Epoch 13/15\n",
      "20/20 [==============================] - 3s 155ms/step - loss: 0.3941 - accuracy: 0.8253 - val_loss: 0.4326 - val_accuracy: 0.7702\n",
      "Epoch 14/15\n",
      "20/20 [==============================] - 3s 155ms/step - loss: 0.3735 - accuracy: 0.8378 - val_loss: 0.4207 - val_accuracy: 0.7794\n",
      "Epoch 15/15\n",
      "20/20 [==============================] - 3s 154ms/step - loss: 0.3554 - accuracy: 0.8462 - val_loss: 0.4111 - val_accuracy: 0.7880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fd0bbc6cc0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization_18 (TextV (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the result in TensorBoard. TensorBoard is part of the ecosystems of TensorFlow, which provides the visualization and tooling needed for machine learning experimentation. It has many functions including:\n",
    "\n",
    "   - tracking and visualizing metrics such as loss and accuracy\n",
    "   - visualizing the model graph (ops and layers)\n",
    "   - viewing histograms of weights, biases, or other tensors as they change over time\n",
    "   - projecting embeddings to a lower dimensional space\n",
    "   - displaying images, text, and audio data\n",
    "   - profiling TensorFlow programs\n",
    "   - and much more!\n",
    "\n",
    "We will not have a systematic introduction here for Tensorboard, but we will give an overview of how it works intuitively. TensorFlow has very detailed documentation on this topic.\n",
    "\n",
    "Once we create the TensorBoard, we will see a few tabs:\n",
    "\n",
    "   - the **Scalars dashboard** shows how the loss and metrics change with every epoch. One can use it to also track training speed, learning rate, and other scalar values;\n",
    "   - the **Graphs dashboard** helps us visualize your model. In this case, the Keras graph of layers is shown which can help us ensure it is built correctly;\n",
    "   - the **Distributions dashboard** and **Histograms dashboard** show the distribution of a Tensor over time. This can be useful to visualize weights and biases and verify that they are changing in an expected way.\n",
    "\n",
    "Additional TensorBoard plugins are automatically enabled when we log other types of data. For example, the Keras TensorBoard callback facility lets us log images and embeddings as well. We can see what other plugins are available in TensorBoard by clicking on the \"inactive\" dropdown towards the top right.\n",
    "\n",
    "We can start the TensorBoard through the command line or within a jupyter notebook experience. The two interfaces are generally the same. In notebooks, we can use the %tensorboard line magic. On the command line, we should run the same command without \"%\". Let's go back to our example and create a TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "C:\\Users\\GAO\\.conda\\envs\\gao_uat\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
       "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
       "C:\\Users\\GAO\\.conda\\envs\\gao_uat\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
       "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
       "C:\\Users\\GAO\\.conda\\envs\\gao_uat\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
       "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
       "C:\\Users\\GAO\\.conda\\envs\\gao_uat\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
       "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
       "C:\\Users\\GAO\\.conda\\envs\\gao_uat\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
       "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
       "C:\\Users\\GAO\\.conda\\envs\\gao_uat\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
       "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
       "2021-10-18 09:35:08.464466: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
       "2021-10-18 09:35:08.466274: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
       "Traceback (most recent call last):\n",
       "  File \"C:\\Users\\GAO\\.conda\\envs\\gao_uat\\Scripts\\tensorboard-script.py\", line 6, in <module>\n",
       "    from tensorboard.main import run_main\n",
       "  File \"C:\\Users\\GAO\\.conda\\envs\\gao_uat\\lib\\site-packages\\tensorboard\\main.py\", line 40, in <module>\n",
       "    from tensorboard import default\n",
       "  File \"C:\\Users\\GAO\\.conda\\envs\\gao_uat\\lib\\site-packages\\tensorboard\\default.py\", line 39, in <module>\n",
       "    from tensorboard.plugins.beholder import beholder_plugin_loader\n",
       "  File \"C:\\Users\\GAO\\.conda\\envs\\gao_uat\\lib\\site-packages\\tensorboard\\plugins\\beholder\\__init__.py\", line 22, in <module>\n",
       "    from tensorboard.plugins.beholder.beholder import Beholder\n",
       "  File \"C:\\Users\\GAO\\.conda\\envs\\gao_uat\\lib\\site-packages\\tensorboard\\plugins\\beholder\\beholder.py\", line 199, in <module>\n",
       "    class BeholderHook(tf.estimator.SessionRunHook):\n",
       "  File \"C:\\Users\\GAO\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\lazy_loader.py\", line 62, in __getattr__\n",
       "    module = self._load()\n",
       "  File \"C:\\Users\\GAO\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\lazy_loader.py\", line 45, in _load\n",
       "    module = importlib.import_module(self.__name__)\n",
       "  File \"C:\\Users\\GAO\\.conda\\envs\\gao_uat\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
       "    return _bootstrap._gcd_import(name[level:], package, level)\n",
       "  File \"C:\\Users\\GAO\\.conda\\envs\\gao_uat\\lib\\site-packages\\tensorflow_estimator\\__init__.py\", line 8, in <module>\n",
       "    from tensorflow_estimator._api.v1 import estimator\n",
       "  File \"C:\\Users\\GAO\\.conda\\envs\\gao_uat\\lib\\site-packages\\tensorflow_estimator\\_api\\v1\\estimator\\__init__.py\", line 8, in <module>\n",
       "    from tensorflow_estimator._api.v1.estimator import experimental\n",
       "  File \"C:\\Users\\GAO\\.conda\\envs\\gao_uat\\lib\\site-packages\\tensorflow_estimator\\_api\\v1\\estimator\\experimental\\__init__.py\", line 8, in <module>\n",
       "    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\n",
       "  File \"C:\\Users\\GAO\\.conda\\envs\\gao_uat\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\__init__.py\", line 25, in <module>\n",
       "    import tensorflow_estimator.python.estimator.estimator_lib\n",
       "  File \"C:\\Users\\GAO\\.conda\\envs\\gao_uat\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator_lib.py\", line 27, in <module>\n",
       "    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\n",
       "  File \"C:\\Users\\GAO\\.conda\\envs\\gao_uat\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\dnn.py\", line 28, in <module>\n",
       "    from tensorflow.python.keras.layers import normalization_v2 as keras_norm\n",
       "ImportError: cannot import name 'normalization_v2' from 'tensorflow.python.keras.layers' (C:\\Users\\GAO\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\layers\\__init__.py)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's retrieve the word embeddings learned during training. The embeddings are weights of the Embedding() layer in the model. The weights matrix is of shape (_vocab\\_size_, _embedding\\_dimension_).\n",
    "\n",
    "We can obtain the weights from the model using get_layer() and get_weights(). In addition, the get_vocabulary() function provides the vocabulary to build a metadata file with one token per line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the weights to disk. To use the 'Embedding Projector', we will need to upload two files in tab separated format: a file of vectors (containing the embedding), and a file of meta data (containing the words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually visualize the embedding on the TensorFlow Embedding Projector: http://projector.tensorflow.org/. To do so, we can open the 'Embedding Projector' (this can also run in a local TensorBoard instance) and click on \"Load data\". Then upload the two files we created above: vecs.tsv and meta.tsv. The embeddings we have trained will now be displayed. We can search for words to find their closest neighbors. For example, try searching for \"beautiful\", we may see neighbors like \"wonderful\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "\n",
    "   - https://www.tensorflow.org/text/guide/word_embeddings#:~:text=The%20Embedding%20layer%20takes%20the,batch%2C%20sequence%2C%20embedding)%20\n",
    "   - https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory\n",
    "   - https://www.tensorflow.org/tutorials/keras/text_classification \n",
    "   - https://www.tensorflow.org/guide/data_performance \n",
    "   - https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\n",
    "   - https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D \n",
    "   - https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten \n",
    "   - https://www.tensorflow.org/tensorboard/get_started \n",
    "   - https://medium.com/analytics-vidhya/understanding-embedding-layer-in-keras-bbe3ff1327ce \n",
    "   - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d37253acc543e0c9e14932998f9d9ba4009fa4cedfab66a675fa2ad6c38d5a1e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
