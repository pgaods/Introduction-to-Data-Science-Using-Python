{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mini-lecture, we study local surrogate models (LIME)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import os \n",
    "import pprint\n",
    "import copy\n",
    "import lime\n",
    "\n",
    "# import eli5\n",
    "# !pip install scikit-image\n",
    "\n",
    "import sklearn.metrics\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import skimage.segmentation\n",
    "\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from lime import lime_tabular\n",
    "from lime import lime_image\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets.base import Bunch\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"C:\\\\Users\\\\gao\\\\GAO_Jupyter_Notebook\\\\Datasets\"\n",
    "os.chdir(path)\n",
    "\n",
    "#path=\"C:\\\\Users\\\\pgao\\\\Documents\\\\PGZ Documents\\\\Programming Workshop\\\\PYTHON\\\\Open Courses on Python\\\\Udemy Course on Python\\Introduction to Data Science Using Python\\\\datasets\"\n",
    "#os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Theory on LIME (Locally Interpretable Model-Agnostic Explanations)\n",
    "\n",
    "In the past we have studied two global methods of model interpretation: PFI and global surrogate method. In this lecture we focus on local surrogate method, specifically, an algorithm called **locally interpretable model-agnostic explanations (LIME)**. Instead of training a global surrogate model, LIME focuses on training local surrogate models to explain individual predictions.\n",
    "\n",
    "The idea of LIME is quite intuitive. First, forget about the training data and imagine you only have the blackbox model where you can input data points and get the predictions of the model. You can probe the box as often as you want. Your goal is to understand why the machine learning model made a certain prediction. LIME tests what happens to the predictions when you give variations of your data into the machine learning model. LIME generates a new dataset consisting of perturbed samples and the corresponding predictions of the blackbox model. On this new dataset, LIME trains an interpretable model, which is weighted by the proximity of the sampled instances to the instance of interest. The interpretable model can be any of those off-the-shelves method such as linear models or decision trees. The learned model should be a good approximation of the machine learning model predictions locally, but it does not have to be a good global approximation. This kind of accuracy is also called **local fidelity**.\n",
    "\n",
    "Here are the recipes for training local surrogate models:\n",
    "\n",
    "   - select your instance of interest for which you want to have an explanation of its blackbox prediction;\n",
    "   - perturb your dataset and get the blackbox predictions for these new points;\n",
    "   - weight the new samples according to their proximity to the instance of interest;\n",
    "   - train a weighted, interpretable model on the dataset with the variations;\n",
    "   - explain the prediction by interpreting the local model. \n",
    "   \n",
    "How do you get the variations of the data? This depends on the type of data, which can be either text, image or tabular data. For texts and images, the solution is to turn single words or super-pixels on or off. In the case of tabular data, LIME creates new samples by perturbing each feature individually, drawing from a normal distribution with mean and standard deviation taken from the feature (assuming numeric features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. LIME on Tabular Data\n",
    "\n",
    "Tabular data is data that comes in tables, with each row representing an instance and each column a feature. LIME samples are not taken around the instance of interest, but from the training data's mass center. But it increases the probability that the result for some of the sample points predictions differ from the data point of interest and that LIME can learn at least some explanation.\n",
    "\n",
    "It is best to visually explain how sampling and local model training works. Below are four pictures that show the following:\n",
    "\n",
    "   - A) random forest predictions given features $x_{1}$ and $x_{2}$; predicted classes $y$: 1 (dark) or 0 (light);\n",
    "   - B) instance of interest (big dot) and data sampled from a normal distribution (small dots);\n",
    "   - C) assignment of higher weight to points near the instance of interest;\n",
    "   - D) signs of the grid showing the classifications of the locally learned model from the weighted samples, with the white line marking the decision boundary ($Pr(y=1) = 0.5$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"LIME tabular.PNG\", height=450, width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a meaningful neighborhood around a point is difficult. This is the biggest critique of LIME actually, which currently uses an exponential smoothing kernel to define the neighborhood. A smoothing kernel is a function that takes two data instances and returns a proximity measure. The kernel width determines how large the neighborhood is: a small kernel width means that an instance must be very close to influence the local model, a larger kernel width means that instances that are farther away also influence the model. The original Python's LIME implementation uses an exponential smoothing kernel (on the normalized data) and the kernel width is 0.75 times the square root of the number of columns of the training data as the default value. Of course, there is no 'correct' answer here. Choosing the right kernel is a big challenge in LIME implementations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. LIME on Texts\n",
    "\n",
    "LIME for texts differs from LIME for tabular data. Variations of the data are generated differently: starting from the original texts, new texts are created by randomly removing words from the original text. The dataset is represented with binary features for each word. A feature is 1 if the corresponding word is included and 0 if it has been removed.\n",
    "\n",
    "Let's see an example where we classify YouTube comments as spam or normal. Let's suppose we have a blackbox model trained on the document word matrix. Each comment is one document (one row) and each column is the number of occurrences of a given word. Short decision trees can be applied and are easy to understand, but in this case the model is complex. In some cases, there could have been a recurrent neural network or a support vector machine trained on word embeddings (abstract vectors). Let us look at the two comments of this dataset and the corresponding classes (1 for spam, 0 for normal comment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"youtube comment snippet.PNG\", height=450, width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create some variations of the datasets used in a local model. For example, some variations of one of the comments may look like this through different types of perturbations below. Each column corresponds to one word in the sentence. Each row is a variation; 1 means that the word is part of this variation and 0 means that the word has been removed. The \"prob\" column shows the predicted probability of spam for each of the sentence variations. The \"weight\" column shows the proximity of the variation to the original sentence, calculated as 1 minus the proportion of words that were removed, for example if 1 out of 7 words was removed, the proximity is $1 - \\frac{1}{7} = 0.86$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"youtube comment perturbation.PNG\", height=450, width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. LIME on Images\n",
    "\n",
    "LIME for images works differently than LIME for tabular data and text. Intuitively, it would not make much sense to perturb individual pixels, since many more than one pixel contribute to one class. Randomly changing individual pixels would probably not change the predictions by much. Therefore, variations of the images are created by segmenting the image into \"superpixels\" and turning superpixels off or on. Superpixels are interconnected pixels with similar colors and can be turned off by replacing each pixel with a user-defined color such as gray (you can think of superpixels as 'blobs' of similar color). The user can also specify a probability for turning off a superpixel in each permutation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are certainly advantages and shortcomings of using LIME. Here are the advantages of using LIME:\n",
    "\n",
    "   1. LIME is very flexible. Even if we replace the underlying machine learning model, you can still use the same local, interpretable model for explanation. Suppose the people looking at the explanations understand decision trees best. Because we use local surrogate models, we can use decision trees as explanations without actually having to use a decision tree to make the predictions. For example, we can use a SVM; and if it turns out that an xgboost model works better, we can replace the SVM and still use as decision tree to explain the predictions. \n",
    "   2. When using surrogate models such as lasso or short trees, the resulting explanations are short (selective) and possibly contrastive. Therefore, they make human-friendly explanations. This is true for tabular data, texts and images.\n",
    "   3. The fidelity measure (how well the interpretable model approximates the blackbox predictions) gives us a good idea of how reliable the interpretable model is in explaining the blackbox predictions in the neighborhood of the data instance of interest.\n",
    "   4. The explanations created with local surrogate models can use other (interpretable) features than the original model was trained on. Of course, these interpretable features must be derived from the data instances. A text classifier can rely on abstract word embeddings as features, but the explanation can be based on the presence or absence of words in a sentence. A regression model can rely on a non-interpretable transformation of some attributes, but the explanations can be created with the original attributes. For example, the regression model could be trained on components of a principal component analysis (PCA) of answers to a survey, but LIME might be trained on the original survey questions. Using interpretable features for LIME can be a big advantage over other methods, especially when the model was trained with non-interpretable features.\n",
    "\n",
    "Here are the disadvantages:\n",
    "\n",
    "   1. The correct definition of the neighborhood is a big, unsolved problem when using LIME with tabular data. This is perhaps the biggest problem of LIME. Really, the explanations depend on the selection of many hyperparameters.\n",
    "   2. Sampling could be improved in the current implementation of LIME. Data points are sampled from a Gaussian distribution, ignoring the correlation between features. This can lead to unlikely data points which can then be used to learn local explanation models.\n",
    "   3. Another really big problem is the instability of the explanations (a.k.a. the explanations are not robust). In an article written by Alvarez-Melis et al. (2018), the authors showed that the explanations of two very close points varied greatly in a simulated setting. Also, in my experience, if you repeat the sampling process, then the explantions that come out can be different. Instability means that it is difficult to trust the explanations, and you should be very critical.\n",
    "   4. All the surrogate models may suffer from low fidelity problem (that is, we have a surrogate model not approximating the blackbox model well enough). \n",
    "   \n",
    "Famous Python packages of LIME includes 'lime' (the original library) and 'eli5' as of 2021. Let's see some real examples now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. LIME for Tabular Data (Multiclass Classfication)\n",
    "\n",
    "In this example, we use a wine-quality dataset related to red and white variants of the Portuguese \"Vinho Verde\" wine. For more details, refer to Cortez et al. (2009). The dataset can be used for classification or regression tasks and it's retrievable in Kaggle. The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones), thus the dataset can also be used for outlier detection algorithms to detect the few excellent or poor wines in standard ML classes. Also, there are many input variables which may be irrelevant. So it could be interesting to test feature selection methods. Here, we implement a classification example using random forests, and then try to use LIME to explain the model. \n",
    "\n",
    "Here are the input variables (based on physicochemical tests):\n",
    "\n",
    "   1. fixed acidity\n",
    "   2. volatile acidity\n",
    "   3. citric acid\n",
    "   4. residual sugar\n",
    "   5. chlorides\n",
    "   6. free sulfur dioxide\n",
    "   7. total sulfur dioxide\n",
    "   8. density\n",
    "   9. pH\n",
    "   10. sulphates\n",
    "   11. alcohol\n",
    "\n",
    "There are only one output variable (based on sensory data): quality (score between 0 and 10). This will be a multiclass classification example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Data Preprocessing\n",
    "\n",
    "Let's first read in the two datasets, one containing white wine information and the other containing the red wine information. Later we will combine them into one dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winewhite = pd.read_csv('winequality-white.csv', sep=';')\n",
    "winewhite.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winered = pd.read_csv('winequality-red.csv', sep=';')\n",
    "winered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winered['type']='red'\n",
    "winewhite['type']='white'\n",
    "\n",
    "wine=pd.concat([winered, winewhite], ignore_index=True)\n",
    "print(wine.info())\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the field 'quality' is an integer. Let's change it to a categorical variable. Notice below that the 'dtype' for this variable is 'categorical', yet the specific value of the column is still expressed as an integer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine['quality']=wine['quality'].astype('category') # changing the 'quality' field to categorical\n",
    "print(type(wine['quality'][3])) # specific value still as an integer (int64)\n",
    "wine.info() # Dtype = 'category'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some descriptive statistics. We will also check how many missing values are there for each variable. And then we will look at correlations between variable through a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.isna().sum() # checking how many missing values are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(wine.corr(), annot=True, robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is no missing variable. In addition, nothing is super highly correlated in this case we don't have to worry about multicollinearity too much for now. Next, let's do some in-depth visual analysis. We will start from univariate analysis, then move onto bivariate analysis and then finally multivariate analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Univariate Visualization \n",
    "\n",
    "Let's first look at continuous variables. Remember that the best way to visualize the continuous variables are through histograms and density plots. We first look at histograms for each variable. Then we will pick the 'sulphates' variable specifically to look at its density plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.hist(bins=15, color='steelblue', edgecolor='black', linewidth=1.0, xlabelsize=8, ylabelsize=8, grid=False)\n",
    "plt.tight_layout(rect=(0, 0, 1.2, 1.2))  # command to give space  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (4, 2))\n",
    "title = fig.suptitle(\"Sulphates Content in Wine\", fontsize=14)\n",
    "fig.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "ax1 = fig.add_subplot(1,1, 1)\n",
    "ax1.set_xlabel(\"Sulphates\")\n",
    "ax1.set_ylabel(\"Density\") \n",
    "sns.kdeplot(wine['sulphates'], ax=ax1, shade=True, color='darkgreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at discrete variables. Since 'quality' is discrete and it indicates wines' low-medium-high quality based on a scale of 10 (all subjective values), let's look at their distribution using barplots. Below, we see that the quality in general is mediocre (6). There are very few good wines (>=9). None of the wines are rated below or equal to 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wine.quality.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (4, 2))\n",
    "title = fig.suptitle(\"Wine Quality Frequency\", fontsize=14)\n",
    "fig.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "ax = fig.add_subplot(1,1, 1)\n",
    "ax.set_xlabel(\"Quality\")\n",
    "ax.set_ylabel(\"Frequency\") \n",
    "w_q = wine['quality'].value_counts()\n",
    "w_q = (list(w_q.index), list(w_q.values))\n",
    "ax.tick_params(axis='both', which='major', labelsize=8.5)\n",
    "bar = ax.bar(w_q[0], w_q[1], color='darkgreen', edgecolor='yellow', linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Bivariate Visualization \n",
    "\n",
    "Now let's do some bivariate analysis. Let’s look at some ways in which we can visualize two continuous, numeric attributes as a start. Scatter plots and joint plots in particular are good ways to not only check for patterns or relationships, but also they also help us see the individual distributions for the attributes; another set of similar techniques include violin plots, which are effective ways to visualize grouped numeric data using kernel density plots (depicting probability density of the data at different values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(wine['sulphates'], wine['alcohol'], color='darkorange', alpha=0.4, edgecolors='w')\n",
    "\n",
    "plt.xlabel('Sulphates')\n",
    "plt.ylabel('Alcohol')\n",
    "plt.title('Wine Sulphates - Alcohol Content', y=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax) = plt.subplots(1, 1, figsize=(8, 3))\n",
    "f.suptitle('Wine Quality - Sulphates Content', fontsize=12)\n",
    "\n",
    "sns.violinplot(x=\"quality\", y=\"sulphates\", data=wine,  ax=ax)\n",
    "ax.set_xlabel(\"Wine Quality\",size = 10,alpha=0.8)\n",
    "ax.set_ylabel(\"Wine Sulphates\",size = 10,alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 2 discrete variables, we can create barplots like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10, 4))\n",
    "title = fig.suptitle(\"Wine Type - Quality\", fontsize=11)\n",
    "fig.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "ax1 = fig.add_subplot(1,2, 1)\n",
    "ax1.set_title(\"Red Wine\")\n",
    "ax1.set_xlabel(\"Quality\")\n",
    "ax1.set_ylabel(\"Frequency\") \n",
    "\n",
    "rw_q = winered['quality'].value_counts()\n",
    "rw_q = (list(rw_q.index), list(rw_q.values))\n",
    "\n",
    "ax1.set_ylim([0, 2500])\n",
    "ax1.tick_params(axis='both', which='major', labelsize=7.5)\n",
    "bar1 = ax1.bar(rw_q[0], rw_q[1], color='maroon', edgecolor='black', linewidth=1)\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.set_title(\"White Wine\")\n",
    "ax2.set_xlabel(\"Quality\")\n",
    "ax2.set_ylabel(\"Frequency\") \n",
    "\n",
    "ww_q = winewhite['quality'].value_counts()\n",
    "ww_q = (list(ww_q.index), list(ww_q.values))\n",
    "\n",
    "ax2.set_ylim([0, 2500])\n",
    "ax2.tick_params(axis='both', which='major', labelsize=7.5)\n",
    "bar2 = ax2.bar(ww_q[0], ww_q[1], color='beige', edgecolor='black', linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. High-Dimensional Visualization \n",
    "\n",
    "Visualizing data till two-dimensions is pretty straightforward but it starts becoming complex as the number of dimensions begin to increase. For three-dimensional data, we can introduce a fake notion of depth by taking a z-axis in our chart or leveraging subplots and facets. However for data higher than three-dimensions, it becomes even more difficult to visualize. The best way to go higher than three dimensions is to use facets, color, shapes, sizes, depth and so on to distinguish data by different 'layers' or 'categories'. \n",
    "\n",
    "Let's look at strategies for visualizing three continuous, numeric attributes. One way would be to have two dimensions represented as the regular length (x-axis) and breadth (y-axis) and also take the notion of depth (z-axis) for the third dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "xs = wine['residual sugar']\n",
    "ys = wine['fixed acidity']\n",
    "zs = wine['alcohol']\n",
    "ax.scatter(xs, ys, zs, s=50, alpha=0.6, edgecolors='w', c='red')\n",
    "\n",
    "ax.set_xlabel('Residual Sugar')\n",
    "ax.set_ylabel('Fixed Acidity')\n",
    "ax.set_zlabel('Alcohol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The picture is nice but it could be done better. Remember that faceting is the process of removing parts of a polygon, polyhedron or polytope, without creating any new vertices. Here, a better option would be to use the notion of faceting as the third dimension (essentially subplots) where each subplot indicates a specific bin from our third variable (dimension). Since we have three continuous variables, we will need to create our own bins manually if we are using the scatterplot functionality from 'matplotlib' as opposed to 'seaborn' (see the following example). The plot below clearly tells us that higher the residual_sugar levels and the alcohol content, the lower the fixed_acidity in the wine samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_list = [0, 0.25, 0.5, 0.75, 1.0]\n",
    "quantile_labels = ['0', '25', '50', '75']\n",
    "wine['res_sugar_labels'] = pd.qcut(wine['residual sugar'], \n",
    "                                    q=quantile_list, labels=quantile_labels)\n",
    "wine['alcohol_levels'] = pd.qcut(wine['alcohol'], \n",
    "                                    q=quantile_list, labels=quantile_labels)\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(wine, col=\"res_sugar_labels\", hue='alcohol_levels')\n",
    "g.map(plt.scatter, \"fixed acidity\", \"alcohol\", alpha=0.7)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize 3 discrete categorical attributes. The trick is to use 'hues':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = sns.catplot(x=\"quality\", hue=\"type\", col=\"res_sugar_labels\", data=wine, kind=\"count\",palette={\"red\": \"#BF9930\", \"white\": \"#CC3188\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using hues is very important in multidimensional data visualization in general. For example, in 3-D case, we can use hues for one of the categorical attributes while using conventional visualizations like scatter plots for visualizing two dimensions for numeric attributes. Notice that below, 'type' is categorical, yet the rest of the variables are continuous. So we can create some cool pairplots while using 'type' as 'hues':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_t = ['density', 'residual sugar', 'total sulfur dioxide', 'fixed acidity', 'type']\n",
    "pp = sns.pairplot(wine[cols_t], height=1.8, hue = 'type' , aspect=1.8,\n",
    "                  plot_kws=dict(edgecolor=\"k\", linewidth=0.5),\n",
    "                  diag_kind=\"kde\", diag_kws=dict(shade=True))\n",
    "\n",
    "fig = pp.fig  \n",
    "fig.subplots_adjust(top=0.93, wspace=0.5)\n",
    "t = fig.suptitle('Wine Attributes Pairwise Plots', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = sns.lmplot(x='sulphates', y='alcohol', hue='type', \n",
    "                palette={\"white\": \"lemonchiffon\", \"red\": \"firebrick\"},\n",
    "                data=wine, fit_reg=True, legend=True,\n",
    "                scatter_kws=dict(edgecolor=\"k\", linewidth=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hues act as good separators for the categories or groups and while there is no or very weak correlation as observed above, we can still understand from these plots that sulphates are slightly higher for red wines as compared to white. Instead of a scatter plot, we can also use a kernel density plot to understand the data in three dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.kdeplot(winewhite['sulphates'], winewhite['alcohol'],\n",
    "                  cmap=\"YlOrBr\", shade=True, shade_lowest=False)\n",
    "ax = sns.kdeplot(winered['sulphates'], winered['alcohol'],\n",
    "                  cmap=\"Reds\", shade=True, shade_lowest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Model Training\n",
    "\n",
    "Let's train the model. We want to use all attributes to predict wine quality, which are indicated by integers. First, let's convert the 'type' variable using dummy variables, as 'type' are categorical and can take on 'red' or 'white' values. We want to use type as an input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines = pd.get_dummies(wine, columns = ['type'], drop_first=True) # drop_first=True ensures that there is no dummy variable trap\n",
    "print(wines.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we created two pseudo-variables in the previous section: 'res sugar labels' and 'alcohol levels'. We will drop them here and then do the test-train split. Then we use an ExtraTreeClassifier() function to train the model. This is a model based on random forests, which is not a easily-interpretable model. In extremely randomized trees, randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wines.drop(['quality','res_sugar_labels', 'alcohol_levels'], axis=1) # getting rid of the 2 derived variables\n",
    "y = wines['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=428)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a sanity check after the train-test split. And then we will train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check=X_train.describe().loc[['min', 'max', 'mean', 'std'], :]\n",
    "sanity_check.round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier(random_state=428)\n",
    "model.fit(X_train, y_train)\n",
    "scores = model.score(X_test, y_test) # returning the mean accuracy on the given test data and labels\n",
    "print(scores) # in multi-label classification, this is the subset accuracy which is a harsh metric since we require for each sample that each label set be correctly predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Model Interpretation\n",
    "\n",
    "Now let's use the lime_tabular.LimeTabularExplainer() class to explain the model for the 3rd observation. As we will see, the model is 41% confident this record falls in class 4 (low quality wine in this case). Meanwhile, the values of alcohol, chlorides, and fixed acidity etc. decrease the wine’s chance to be classified as medium quality (6):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array(X_train), # this has to be a 2-D numpy array\n",
    "    feature_names=X_train.columns, # list of names (strings) corresponding to the columns in the training data\n",
    "    class_names=list(y.unique()), # list of class names, ordered according to whatever the classifier is using\n",
    "    mode='classification' # classification or regression\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(\n",
    "    data_row=X_test.iloc[2], # picking the 3rd observation\n",
    "    predict_fn=model.predict_proba # prediction function\n",
    ")\n",
    "\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another individual record and see whether we can explain this. This time, we will add another argument _top_labels_, which indicates how many classes in this class needs explanations (in classification problems):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(\n",
    "    data_row=X_test.iloc[9], # picking the 3rd observation\n",
    "    predict_fn=model.predict_proba, # prediction function\n",
    "    top_labels=7\n",
    ")\n",
    "\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. LIME for Tabular Data (Regression)\n",
    "\n",
    "Now let's use the same data but this time we will do a regression exercise. Let's use the same dataset and the 'pH' as a target variable and let all other information predict the target using a neural network model:\n",
    "\n",
    "To start with, let's convert all the categorical variables to dummies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines2 = pd.get_dummies(wine, columns = ['type', 'quality'], drop_first=True) # drop_first=True ensures that there is no dummy variable trap\n",
    "wines2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data and standardize the data since we are planning to use a neural network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wines2.drop(['pH'], axis=1) # getting rid of the target variable in the input\n",
    "y = wines2['pH']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=428)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train=pd.DataFrame(scaler.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "X_test=pd.DataFrame(scaler.fit_transform(X_test), index=X_test.index, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a sanity check on the data after train-test split. Since neural network models require us that we use unit level data, we need to make sure that they all have mean 0 and unit STD after transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check=X_train.describe().loc[['min', 'max', 'mean', 'std'], :]\n",
    "sanity_check.round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(17,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(9,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(17,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train.values,y=y_train.values,\n",
    "          validation_data=(X_test,y_test.values),\n",
    "          batch_size=34,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "print('Model predictions:\\n', predictions[0:5], \"\\n\")\n",
    "print('MAE: ', mean_absolute_error(y_test,predictions))\n",
    "print('Explained Variance Score: ', explained_variance_score(y_test,predictions))\n",
    "print('Root Mean Squared Error (RMSE): ', np.sqrt(mean_squared_error(y_test,predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test,predictions, color='darkorange') # drawing our predictions\n",
    "plt.plot(y_test,y_test,'blue') # perfect predictions are indicated on the blue line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the errors ($y-\\hat{y}$) are well-behaved. And then we will try to explain the model using an individual record. Notice that the explanation is applied to the transformed (standardized) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = y_test.values.reshape(len(y_test), 1)-predictions\n",
    "sns.distplot(errors, color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train.values, feature_names=X_train.columns, class_names=['pH'], verbose=True, mode='regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the 13th data point so we can see how the model predicts it. The predicted value is 3.15 with \"fixed acidity<=-0.63\", \"residual sugar<=-0.77\" poviding the most positive evaluation in favor of the predicted value (keep in mind that all the input values are standardized already). As you see, the interpretation for regression problem is less natural compared to classification problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(\n",
    "    data_row=X_test.iloc[12], # picking the 13th observation\n",
    "    predict_fn=model.predict, # the predict_fn argument is required and the 'top_labels' argument should be ignored\n",
    ")\n",
    "\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_list() # returning a list of tuples (representation, weight), with weight as a float."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. LIME for Text Data\n",
    "\n",
    "Now let's apply LIME to a text data example. In the following example, the text data consists of a total of 1000 posts to two Google newsgroups: \"alt.atheism\" and \"soc.religion.cristian\". The task is to predict which of the two groups a post is from. What makes the data so interesting is that both newsgroups discuss similar themes using the same words, but with an obviously rather different angle.\n",
    "\n",
    "This should actually be a diffifult task for a simplistic \"bag-of-words (BOW)\" style classifier in theory. However, simple classifiers perform remarkably well on this task. This is because this dataset is \"famous\" for the presence of many confounding features, for instance the e-mail domains that are present in the headers. The classifier simply learns that a certain domain is only used in posts in one of the two classes. Therefore, the great out-of-the box performance by simple text classifiers is not indicative of any real-world performance, since it learns to recognize particularities of this dataset. Interpretability is thus essential to understand whether the model is any good or not. \n",
    "\n",
    "With this example, let's build two models. The first one is a BOW TF-IDF model. The second one, we will use Glove word embedding techniques for features (pre-trained in this case), and then using these embeddings as features, we will apply the SVM for prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Data Preprocessing\n",
    " \n",
    "The dataset is built-in one from the 'sklearn' package. First, let's read in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True, \n",
    "                                      categories=categories, remove=[]) #('headers', 'footers', 'quotes')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle=True, \n",
    "                                     categories=categories, remove=[])#('headers', 'footers', 'quotes'))\n",
    "\n",
    "class_names = ['atheism', 'christianity']\n",
    "# print(newsgroups_train.DESCR)\n",
    "type(newsgroups_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to delete empty posts from the trainning and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_empty_posts(data):\n",
    "    \"\"\"\n",
    "    Changes the passed argument in-place by removing elements from data.data\n",
    "    \"\"\"\n",
    "    numdeleted = 0\n",
    "    for i, doc in enumerate(data.data):\n",
    "        if len(doc.split()) < 3:\n",
    "            del data.data[i]\n",
    "            data.target = np.delete(data.target, i)\n",
    "            numdeleted += 1\n",
    "            # print(doc)\n",
    "    print('Deleted {} posts'.format(numdeleted))\n",
    "\n",
    "delete_empty_posts(newsgroups_train)\n",
    "delete_empty_posts(newsgroups_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(newsgroups_train.target_names))\n",
    "print(len(newsgroups_train.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', token_pattern=r'\\b[a-zA-Z]{3,}\\b', lowercase=False,\n",
    "                            min_df=5, max_df=0.7, stop_words='english')\n",
    "\n",
    "#vectorizer_small = TfidfVectorizer(analyzer='word', token_pattern=r'\\b[a-zA-Z]{3,}\\b', lowercase=True, min_df=10, max_df=0.7, stop_words='english') # an alternative to play around with\n",
    "\n",
    "vectorizer.fit_transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important property of LIME is that it is \"model-agnostic\": it just needs an object with a predict_proba() method that returns the probability of the positive class, and the instance that requires explanation. So we can use the whole family of scikit-learn classifiers, but also pipelines, which is a powerful thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(alpha=0.1)\n",
    "pl = make_pipeline(vectorizer, mnb)\n",
    "\n",
    "def test_classifier_performance(clf):\n",
    "    \"\"\"\n",
    "    clf will be fitted on the newsgroup train data, measured on test\n",
    "    clf can be a sklearn pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    clf.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "    pred = clf.predict(newsgroups_test.data)\n",
    "    print('Accuracy: {:.3f}'.format(metrics.accuracy_score(newsgroups_test.target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_grid = np.logspace(-3, 0, 4)#Is smoothing parameter for the counts\n",
    "param_grid = [{'multinomialnb__alpha': alpha_grid }]\n",
    "gs = GridSearchCV(pl, param_grid=param_grid, cv=5, return_train_score=True)\n",
    "\n",
    "gs.fit(newsgroups_train.data, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gs.cv_results_['param_multinomialnb__alpha'].data,gs.cv_results_['mean_test_score'], label='test')\n",
    "plt.plot(gs.cv_results_['param_multinomialnb__alpha'].data,gs.cv_results_['mean_train_score'], label='train')\n",
    "plt.legend()\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Accuracy-score')\n",
    "plt.title('Accuracy TF-IDF, Multinomial NB')\n",
    "plt.semilogx();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an impressively high accuracy, of over 97% on the test data. Only a minimal smoothing of the counts using pseudocounts is needed. Now, let's use LIME to interpret the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=35\n",
    "explainer = LimeTextExplainer(class_names=class_names) \n",
    "pl.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "exp = explainer.explain_instance(newsgroups_test.data[idx], \n",
    "                                 pl.predict_proba, \n",
    "                                 num_features=10)\n",
    "print('Document id: %d' % idx)\n",
    "print('Probability(christian) = {:.3f}'.format(pl.predict_proba([newsgroups_test.data[idx]])[0,1]))\n",
    "print('True class: %s' % class_names[newsgroups_test.target[idx]])\n",
    "print('R2 score: {:.3f}'.format(exp.score))\n",
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to interpret all of this?\n",
    "\n",
    "   - On the left, we see the actual prediction of the base model. It is 95% sure it is a post about Christianity, and it is correct.\n",
    "   - In the middle, we see a graphical display of the most important features to the probability of the prediction, locally. As an example, the word \"Scripture\" (note, we did not lowercase the words when vectorizing, so there is a distinction between \"scripture\" and \"Scripture\".\n",
    "   - At the right side, we see a very helpful graphical display of the text, and where the important features appear.\n",
    "\n",
    "The feature importances, as displayed in the middle, can be interpreted as: the presence of the corresponding word has increased the prediction probability toward the positive class by this fraction.\n",
    "\n",
    "Let's check this, by removing the word \"Scripture\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test_modified = Bunch(data=newsgroups_test.data.copy(), target=newsgroups_test.target)\n",
    "newsgroups_test_modified.data[35] = ' '.join([word for word in newsgroups_test_modified.data[35].split(' ') if not \n",
    "                                    word.startswith('Scripture')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the original and modified version\n",
    "print(newsgroups_test.data[35][0:800])\n",
    "print('*********')\n",
    "print(newsgroups_test_modified.data[35][0:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=35\n",
    "explainer = LimeTextExplainer(class_names=class_names) \n",
    "pl.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "exp = explainer.explain_instance(newsgroups_test_modified.data[idx], \n",
    "                                 pl.predict_proba, \n",
    "                                 num_features=10)\n",
    "print('Document id: %d' % idx)\n",
    "print('Probability(christian) = {:.3f}'.format(pl.predict_proba([newsgroups_test_modified.data[idx]])[0,1]))\n",
    "print('True class: %s' % class_names[newsgroups_test_modified.target[idx]])\n",
    "print('R2 score: {:.3f}'.format(exp.score))\n",
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, the absence of the word \"Scripture\" reduced the probability for Christianity. The genearl question is: can we make our surrogate model perform better? We can influence it in several ways:\n",
    "\n",
    "   - increasing the number of features (obvious measure)'\n",
    "   - changing the model regressors (by default, it is a Ridge regressor with alpha=1.0);\n",
    "   - changing the kernel width that determines the locality (by default, it is 25).\n",
    "\n",
    "To the first point: the number of features is obviously an important parameter. The top-10 might not be enough to build an accurate model.\n",
    "\n",
    "Regarding the model regressor, we might reduce alpha of the Ridge regressor. The default is arbitrarily set at 1.0. Since the R-squared is that of the fit itself, a lower alpha will result in a better fit. But what value is best? This is hard to tell. Increasing the number of samples might be a good thing to reduce the variance of the fit, and getting away with lower regularization.\n",
    "\n",
    "Finally, the value of 25 for the kernel width, that is applied to an exponential kernel on the cosine distance, is arbitrary and somewhat remarkable. Since the cosine distance is bounded between -1 and 1, a value of 25 is practically identical to a uniform weighting. One could make it smaller, but how much smaller should it be?\n",
    "\n",
    "A strategy recommended is to explain the same point and other points multiple time with different random seeds (or without random seeds), and verify that R-squared is consistently above some minimum level. A question that remains, though, is: what is our locality? And what should it be? (kernel size). Again, there is no easy answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. GloVe Model\n",
    "\n",
    "Next, let's apply the Glove (word embedding) model. Recall that a nice aspect of LIME is that it accepts sci-kit learn pipelines, and thus, we can make things a bit more sophisticated, by using word embeddings and a support vector machine.\n",
    "\n",
    "The benefit of using word embeddings is that words with a similar meaning typically get similar vectors. We can map our words to a numerical vector (with a dimensionality that can be one or several orders of magnitude lower compared to our original vocabulary). As a result, we can expect our classifier to generalize better. Note that here we simply average the invidual word vectors. A possible refinement is weighting with TF-IDF weights.\n",
    "\n",
    "We will use the Glove embeddings that were trained on a 6-billion word corpus, and have a dimensionality of 50. These can be downloaded here: https://nlp.stanford.edu/projects/glove/. \n",
    "\n",
    "The following simple class makes it possible to use Glove vectors in a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveVectorizer:\n",
    "    def __init__(self, verbose=False, lowercase=True, minchars=3):\n",
    "        # load in pre-trained word vectors\n",
    "        print('Loading word vectors...')\n",
    "        word2vec = {}\n",
    "        embedding = []\n",
    "        idx2word = []\n",
    "        with open('glove.6B.50d.txt', encoding=\"utf8\") as f:\n",
    "              # is just a space-separated text file in the format:\n",
    "              # word vec[0] vec[1] vec[2] ...\n",
    "              for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vec = np.asarray(values[1:], dtype='float32')\n",
    "                word2vec[word] = vec\n",
    "                embedding.append(vec)\n",
    "                idx2word.append(word)\n",
    "        print('Found %s word vectors.' % len(word2vec))\n",
    "\n",
    "        self.word2vec = word2vec\n",
    "        self.embedding = np.array(embedding)\n",
    "        self.word2idx = {v:k for k,v in enumerate(idx2word)}\n",
    "        self.V, self.D = self.embedding.shape\n",
    "        self.verbose = verbose\n",
    "        self.lowercase = lowercase\n",
    "        self.minchars = minchars\n",
    "\n",
    "    def fit(self, data, *args):\n",
    "        pass\n",
    "\n",
    "    def transform(self, data, *args):\n",
    "        X = np.zeros((len(data), self.D))\n",
    "        n = 0\n",
    "        emptycount = 0\n",
    "        for sentence in data:\n",
    "            # Note: lower-casing the words\n",
    "            if self.lowercase:\n",
    "                tokens = sentence.lower().split()\n",
    "            else:\n",
    "                tokens = sentence.split()\n",
    "            vecs = []\n",
    "            for word in tokens:\n",
    "                if len(word) >= self.minchars and word in self.word2vec:\n",
    "                    vec = self.word2vec[word]\n",
    "                    vecs.append(vec)\n",
    "            if len(vecs) > 0:\n",
    "                vecs = np.array(vecs)\n",
    "                X[n] = vecs.mean(axis=0)\n",
    "            else:\n",
    "                emptycount += 1\n",
    "            n += 1\n",
    "        if self.verbose:\n",
    "            print(\"Number of samples with no words found / total: %s / %s\" % (emptycount, len(data)))\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X, *args):\n",
    "        self.fit(X, *args)\n",
    "        return self.transform(X, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gv = GloveVectorizer()\n",
    "clf_svm = svm.SVC(gamma='scale', probability=True, C=100) # C: slack-variable penalty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make a pipeline with the a word-vectorizing using the GloVe word embeddings, and an SVM model. Before making the predictions, let's first optimize the hyperparameter $C$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p5 = make_pipeline(gv, clf_svm)\n",
    "\n",
    "C_grid = np.logspace(1, 4, 4)\n",
    "param_grid = [{'svc__C': C_grid }] # some basic parameter tuning\n",
    "gs = GridSearchCV(p5, param_grid=param_grid, cv=5, return_train_score=True)\n",
    "\n",
    "gs.fit(newsgroups_train.data, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gs.cv_results_['param_svc__C'].data,gs.cv_results_['mean_test_score'], label='test')\n",
    "plt.plot(gs.cv_results_['param_svc__C'].data,gs.cv_results_['mean_train_score'], label='train')\n",
    "plt.legend()\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.semilogx();\n",
    "#plt.ylim([0.8, 0.82])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = svm.SVC(gamma='scale', probability=True, C=1000) \n",
    "p5 = make_pipeline(gv, clf_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=35\n",
    "explainer = LimeTextExplainer(class_names=class_names,\n",
    "                                kernel_width=1.0,\n",
    "                             random_state=1)\n",
    "p5.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "exp = explainer.explain_instance(newsgroups_test.data[idx], \n",
    "                                 p5.predict_proba, \n",
    "                                 model_regressor=Ridge(alpha=0.01),\n",
    "                                 num_features=25,\n",
    "                                 num_samples=20000)\n",
    "print('Document id: %d' % idx)\n",
    "print('Probability(christian) = {:.3f}'.format(p5.predict_proba([newsgroups_test.data[idx]])[0,1]))\n",
    "print('True class: %s' % class_names[newsgroups_test.target[idx]])\n",
    "print('R2 score: {:.3f}'.format(exp.score))\n",
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. LIME for Image Data\n",
    "\n",
    "One of the most excellent aspect of LIME is that it can also handle image data. In this section, we first illustrate what LIME does using raw Python codes so that we can understand what each step LIME is doing behind the scene. Then we will apply LIME package directly.\n",
    "\n",
    "Let’s start by reading an image and using the pre-trained \"InceptionV3\" model available in Keras to predict the class of such image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xi = skimage.io.imread(\"https://arteagac.github.io/blog/lime_image/img/cat-and-dog.jpg\")\n",
    "Xi = skimage.transform.resize(Xi, (299,299)) \n",
    "Xi = (Xi - 0.5)*2 # inception pre-processing\n",
    "skimage.io.imshow(Xi/2+0.5) # showing image before inception preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(222)\n",
    "inceptionV3_model = tf.keras.applications.inception_v3.InceptionV3() # loading pretrained model\n",
    "preds = inceptionV3_model.predict(Xi[np.newaxis,:,:,:])\n",
    "top_pred_classes = preds[0].argsort()[-5:][::-1] # saving IDs of top 5 classes\n",
    "decode_predictions(preds)[0] # printing top 5 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have:\n",
    "\n",
    "   - Labrador Retriever (82.2%)\n",
    "   - Golden Retriever (1.5%)\n",
    "   - American Staffordshire Terrier (0.9%)\n",
    "   - Bull Mastiff (0.8%)\n",
    "   - Great Dane (0.7%)\n",
    "   \n",
    "With this information, the input image and the pre-trained InceptionV3 model, we can proceed to generate explanations with LIME. In this example we will generate explanations for the class Labrador Retriever.\n",
    "\n",
    "LIME creates explanations by generating a new dataset of random perturbations (with their respective predictions) around the instance being explained and then fitting a weighted local surrogate model. This local model is usually a simpler model with intrinsic interpretability such as a linear regression model. For the first step, let's generate random perturbations for input image. The following script uses the quick-shift segmentation algorithm to compute the super-pixels in the image. In addition, it generates an array of 150 perturbations where each perturbation is a vector with zeros and ones that represent whether the super-pixel is on or off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superpixels = skimage.segmentation.quickshift(Xi, kernel_size=4,max_dist=200, ratio=0.2)\n",
    "num_superpixels = np.unique(superpixels).shape[0]\n",
    "skimage.io.imshow(skimage.segmentation.mark_boundaries(Xi/2+0.5, superpixels))\n",
    "\n",
    "num_perturb = 150 # generating perturbations\n",
    "perturbations = np.random.binomial(1, 0.5, size=(num_perturb, num_superpixels))\n",
    "\n",
    "def perturb_image(img,perturbation,segments): # creating a function to apply perturbations to images\n",
    "    active_pixels = np.where(perturbation == 1)[0]\n",
    "    mask = np.zeros(segments.shape)\n",
    "    for active in active_pixels:\n",
    "        mask[segments == active] = 1 \n",
    "    perturbed_image = copy.deepcopy(img)\n",
    "    perturbed_image = perturbed_image*mask[:,:,np.newaxis]\n",
    "    return perturbed_image\n",
    "\n",
    "print(perturbations[0]) # showing an example of perturbations\n",
    "skimage.io.imshow(perturb_image(Xi/2+0.5,perturbations[0],superpixels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to predict classes for perturbations. The following script uses the \"inceptionV3_model\" to predict the class of each of the perturbed images. The shape of the predictions is (150, 1000) which means that for each of the 150 images, we get the probability of belonging to the 1000 classes in \"InceptionV3\". From these 1000 classes we will use only the \"Labrador\" class in further steps since it is the prediction we want to explain. In this example, 150 perturbations were used. However, for real applications, a larger number of perturbations will produce more reliable explanations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for pert in perturbations:\n",
    "    perturbed_img = perturb_image(Xi,pert,superpixels)\n",
    "    pred = inceptionV3_model.predict(perturbed_img[np.newaxis,:,:,:])\n",
    "    predictions.append(pred)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything to fit a linear model using the perturbations as input features and the predictions for Labrador \"predictions[labrador]\" as output. However, before we fit a local surrogatea model (say linear model), LIME needs to give more weight (importance) to images that are closer to the image being explained. \n",
    "\n",
    "To compute weights (importance) for the perturbations, we use a distance metric to evaluate how far is each perturbation from the original image. The original image is just a perturbation with all the super-pixels active (all elements in one). Given that the perturbations are multidimensional vectors, the cosine distance is a metric that can be used for this purpose. After the cosine distance has been computed, a kernel function is used to translate such distance to a value between zero and one (a weight). At the end of this process we have a weight (importance) for each perturbation in the dataset. Below is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = np.ones(num_superpixels)[np.newaxis,:] # perturbation with all superpixels enabled \n",
    "distances = sklearn.metrics.pairwise_distances(perturbations,original_image, metric='cosine').ravel()\n",
    "print(distances.shape)\n",
    "\n",
    "kernel_width = 0.25 # transforming distances to a value between 0 and 1 using a kernel function\n",
    "weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2)) #Kernel function\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we fit a weighted linear model (the surrogate model) using the information obtained in the previous steps. We get a coefficient for each super-pixel in the image that represents how strong is the effect of the super-pixel in the prediction of 'Labrador':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_explain = top_pred_classes[0] #Labrador class\n",
    "simpler_model = LinearRegression()\n",
    "simpler_model.fit(X=perturbations, y=predictions[:,:,class_to_explain], sample_weight=weights)\n",
    "coeff = simpler_model.coef_[0]\n",
    "\n",
    "num_top_features = 4 # using coefficients from the linear model to extract top features\n",
    "top_features = np.argsort(coeff)[-num_top_features:] \n",
    "\n",
    "mask = np.zeros(num_superpixels)  # showing only the super-pixels corresponding to the top features\n",
    "mask[top_features]= True # activating top super-pixels\n",
    "skimage.io.imshow(perturb_image(Xi/2+0.5,mask,superpixels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what LIME returns as explanation. The area of the image (super-pixels) that have a stronger association with the prediction of 'Labrador Retriever' shown above. This explanation suggests that the pre-trained 'InceptionV3' model is doing a good job predicting the 'Labrador' class for the given image. This example shows how LIME can help to increase confidence in a machine-learning model by understanding why it is returning certain predictions.\n",
    "\n",
    "Now let's actually apply the LIME package. Let's look at a giant pandas image and then apply LIME to explain it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/marcellusruben/All_things_medium/main/Lime/panda_00024.jpg'\n",
    "\n",
    "def transform_img_fn_ori(url):\n",
    "    img = skimage.io.imread(url)\n",
    "    img = skimage.transform.resize(img, (299,299))\n",
    "    img = (img - 0.5)*2\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    preds = inet_model.predict(img)\n",
    "    for i in decode_predictions(preds)[0]:\n",
    "        print(i)\n",
    "    return img\n",
    "\n",
    "inet_model = tf.keras.applications.inception_v3.InceptionV3()\n",
    "images_inc_im = transform_img_fn_ori(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the pre-trained \"InceptionV3\" model also predicts that our image is a giant panda. Now let’s interpret the behavior of our pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation= explainer.explain_instance(images_inc_im[0].astype('double'), inet_model.predict,  top_labels=3, hide_color=0, num_samples=1000)\n",
    "\n",
    "temp_1, mask_1 = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=True)\n",
    "temp_2, mask_2 = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=10, hide_rest=False)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,10))\n",
    "ax1.imshow(mark_boundaries(temp_1, mask_1))\n",
    "ax2.imshow(mark_boundaries(temp_2, mask_2))\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References: \n",
    "\n",
    "#### Interpretable AI:\n",
    "   - https://christophm.github.io/interpretable-ml-book/\n",
    "   \n",
    "#### Overview on LIME:\n",
    "   - Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. \"Why should I trust you?: Explaining the predictions of any classifier.\" Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. ACM (2016).\n",
    "   - Alvarez-Melis, David, and Tommi S. Jaakkola. \"On the robustness of interpretability methods.\" arXiv preprint arXiv:1806.08049 (2018).\n",
    "   - P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553 (2009).\n",
    "   - https://towardsdatascience.com/understanding-model-predictions-with-lime-a582fdff3a3b\n",
    "   - https://towardsdatascience.com/lime-how-to-interpret-machine-learning-models-with-python-94b0e7e4432e\n",
    "   - https://towardsdatascience.com/whats-wrong-with-lime-86b335f34612\n",
    "   - https://github.com/marcotcr/lime\n",
    "\n",
    "#### LIME for Tabular Data:\n",
    "   - https://www.analyticsvidhya.com/blog/2017/06/building-trust-in-machine-learning-models/?utm_source=blog&utm_medium=shapley-value-machine-learning-interpretability-game-theory\n",
    "   - https://github.com/marcotcr/lime/tree/ce2db6f20f47c3330beb107bb17fd25840ca4606\n",
    "   - https://marcotcr.github.io/lime/tutorials/Using%2Blime%2Bfor%2Bregression.html\n",
    "   - https://www.kaggle.com/rajyellow46/wine-quality\n",
    "   \n",
    "#### LIME for Text Data:\n",
    "   - https://donernesto.github.io/blog/explaining-text-classification-predictions-with-lime/\n",
    "   - https://github.com/stanfordnlp/GloVe\n",
    "   \n",
    "#### LIME for Image Data:\n",
    "   - https://towardsdatascience.com/interpretable-machine-learning-for-image-classification-with-lime-ea947e82ca13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
