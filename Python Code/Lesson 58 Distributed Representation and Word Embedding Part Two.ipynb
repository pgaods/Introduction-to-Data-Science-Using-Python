{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mini-lecture, we systematically study the building blocks of the Gensim library. \r\n",
    "\r\n",
    "In the world of NLP, NLTK is the most dominant foundational package. But there are other important packages. The Gensim library is another powerful package that can handle a variety of tasks, such latent semantic indexing as well as other topic extraction. It can also employ models based on deep learning techniques such as word embeddings. We have seen the functionalities of NLTK before; in this mini-lecture we explore basic concepts in the Gensim library. Majority of the materials come from the official Gensim documentation. \r\n",
    "\r\n",
    "In this tutorial, we will use the stable version of Gensim 3.8.3. Currently Gensim is going through some big changes for versions above 4.0.1. The new version is more powerful but a bit unstable. So we will use the stable version in this tutorial. Details of the upgrade can be found here:\r\n",
    "\r\n",
    "   - https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import gensim \n",
    "import pprint\n",
    "import logging\n",
    "\n",
    "from collections import defaultdict\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim version:  3.8.3\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\r\n",
    "\r\n",
    "path=\"C:\\\\Users\\\\GAO\\\\python workspace\\\\GAO_Jupyter_Notebook\\\\Datasets\"\r\n",
    "os.chdir(path)\r\n",
    "\r\n",
    "print('Gensim version: ', gensim.__version__)\r\n",
    "\r\n",
    "# path=\"C:\\\\Users\\\\pgao\\\\Documents\\\\PGZ Documents\\\\Programming Workshop\\\\PYTHON\\\\Open Courses on Python\\\\Udemy Course on Python\\Introduction to Data Science Using Python\\\\datasets\"\r\n",
    "# os.chdir(path)\r\n",
    "\r\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Core Concepts in Gensim\n",
    "\n",
    "We will first go over some of the core concepts in this library before plodding forward with other word embedding methods.\n",
    "\n",
    "The core concepts of Gensim library are:\n",
    "\n",
    "   1. **(Gensim) document**: some text.\n",
    "   2. **(Gensim) corpus**: a collection of documents.\n",
    "   3. **(Gensim) vector**: a mathematically convenient representation of a document.\n",
    "   4. **(Gensim) model**: an algorithm for transforming vectors from one representation to another.\n",
    "\n",
    "A (Gensim) document is an object of the text sequence type (commonly known as 'str' in Python 3). A document could be anything from a short 140 character tweet, a single paragraph (i.e., journal article abstract), a news article, or a book. Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document=\"Human machine interface for lab and computer applications.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A (Gensim) corpus is a collection of document objects. The distinction between a document and a vector is that the former is text, and the latter is a mathematically convenient representation of the text (say using a list or a dictionary etc.). Corpora serve two roles in Gensim:\r\n",
    "\r\n",
    "   1. Input for training a (Gensim) model object: during training, the models use this training corpus to look for common themes and topics, initializing their internal model parameters. Gensim focuses on unsupervised models so that no human intervention, such as costly annotations or tagging documents by hand, is required.\r\n",
    "   2. (Gensim) documents to organize: after training, a topic model can be used to extract topics from new documents (documents not seen in the training corpus). Such corpora can be indexed for an operation in Gensim called **(Gensim) similarity queries**.\r\n",
    "\r\n",
    "Here is an example of a corpus. It consists of 9 documents, where each document is a string consisting of a single sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a particularly small example of a corpus for illustration purposes. Another example could be a list of all the plays written by Shakespeare, list of all wikipedia articles, or all tweets by a particular person of interest.\n",
    "\n",
    "After collecting our corpus, there are typically a number of preprocessing steps we want to undertake. For now, we will keep it simple and just remove some commonly used English words (such as 'the') and words that occur only once in the corpus. In the process of doing so, we’ll tokenize our data. Tokenization breaks up the documents into words (in this case using space as a delimiter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:\n",
      "  [['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'], ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'management', 'system'], ['system', 'human', 'system', 'engineering', 'testing', 'eps'], ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'], ['generation', 'random', 'binary', 'unordered', 'trees'], ['intersection', 'graph', 'paths', 'trees'], ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'], ['graph', 'minors', 'survey']] \n",
      "\n",
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "mini_stoplist = set('for a of the and to in'.split(' ')) # creating a set of frequent words, which is a subset of the NLTK's stopwords\n",
    "\n",
    "texts = [[word for word in document.lower().split() if word not in mini_stoplist] for document in text_corpus] # lowercasing each document and splitting it by white space and filtering out elements from mini_stopwords\n",
    "print(\"texts:\\n \", texts, \"\\n\")\n",
    "\n",
    "frequency = defaultdict(int) # counting word frequencies\n",
    "for t in texts:\n",
    "    for token in t:\n",
    "        frequency[token] += 1 \n",
    "\n",
    "processed_corpus = [[token for token in t if frequency[token] > 1] for t in texts] # only keeping words that appear more than once\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, we want to associate each word in the corpus with a unique integer ID. We can do this using the gensim.corpora.Dictionary() class. This method defines the vocabulary of all words that our processing knows about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n",
      "first element of the dictionary object:  computer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gensim.corpora.dictionary.Dictionary"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)\n",
    "print(\"first element of the dictionary object: \", dictionary[0])\n",
    "type(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To infer the latent structure in our corpus we need a way to represent documents that we can manipulate mathematically. One approach is to represent each document as a vector of features. For example, a single feature may be thought of as a question-answer pair:\r\n",
    "\r\n",
    "   - How many times does the word 'splonge' appear in the document? Zero.\r\n",
    "   - How many paragraphs does the document consist of? Two.\r\n",
    "   - How many fonts does the document use? Five.\r\n",
    "\r\n",
    "The question is usually represented only by its integer IDs (such as 1, 2 and 3). The representation of this document then becomes a series of pairs like (1, 0.0), (2, 2.0), (3, 5.0). This is known as a **(Gensim) dense vector**, because it contains an explicit answer to each of the above questions. If we know all the questions in advance, we may leave them implicit and simply represent the document as (0, 2, 5). This sequence of answers is the vector for our document (in this case a 3-dimensional dense vector). For practical purposes, only questions to which the answer is (or can be converted to) a single floating point number are allowed in Gensim.\r\n",
    "\r\n",
    "In practice, vectors often consist of many zero values. To save memory, Gensim omits all vector elements with value 0.0. The above example thus becomes (2, 2.0), (3, 5.0). This is known as a **(Gensim) sparse vector** or **(Gensim) bag-of-words vector**. The values of all missing features in this sparse representation can be unambiguously resolved to zero.\r\n",
    "\r\n",
    "Conceptually, assuming the questions are the same, we can compare the vectors of two different documents to each other. For example, assume we are given two vectors (0.0, 2.0, 5.0) and (0.1, 1.9, 4.9). Because the vectors are very similar to each other, we can conclude that the documents corresponding to those vectors are similar, too. Of course, the correctness of that conclusion depends on how well we picked the questions in the first place. This is a hard topic because it involves measuring dissimilarities between two bulks of texts. But for now, that intuition based on the vector value makes sense certainly in many situations. \r\n",
    "\r\n",
    "Another approach to represent a document as a vector is the bag-of-words (BOW) model. Under the BOW model, each document is represented by a vector containing the frequency counts of each word in the dictionary. For example, assume we have a dictionary containing the words ['coffee', 'milk', 'sugar', 'spoon']. A document consisting of the string 'coffee milk coffee' would then be represented by the vector [2, 1, 0, 0] where the entries of the vector are (in order) the occurrences of 'coffee', 'milk', 'sugar' and 'spoon' in the document. The length of the vector is the number of entries in the dictionary. \r\n",
    "\r\n",
    "In our example, our processed corpus has 12 unique words in it, which means that each document will be represented by a 12-dimensional vector under the bag-of-words model. We can use the dictionary to turn tokenized documents into these 12-dimensional vectors. We can see what these IDs correspond to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0,\n",
      " 'eps': 8,\n",
      " 'graph': 10,\n",
      " 'human': 1,\n",
      " 'interface': 2,\n",
      " 'minors': 11,\n",
      " 'response': 3,\n",
      " 'survey': 4,\n",
      " 'system': 5,\n",
      " 'time': 6,\n",
      " 'trees': 9,\n",
      " 'user': 7}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted to vectorize the phrase \"Human computer interaction\" (note that this phrase was not in our original corpus). We can create the bag-of-word representation for a document using the doc2bow() method of the dictionary, which simply counts the number of occurrences of each distinct word, converts the word to its integer word ID and returns a sparse representation of the word counts. The first entry in each tuple corresponds to the ID of the token in the dictionary, the second corresponds to the count of this token: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3), (1, 3), (7, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human vs computer and computer vs computer and human vs human interaction are all confusing. Users will not like this idea, but this one particular user figured out everything!\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember in the dictionary defined before, we see that three words reside in that dictionary: computer (with the token ID equal to 0), human (indexed by the token ID 1), and user (indicated by the token ID 7). These three words occur in the next text named 'new_doc'. The object 'new_vec' simply tells us the corresponding word frequencies. Note that the word 'vs' did not occur in the original corpus and so it was not included in the vectorization. Also note that this vector only contains entries for words that actually appeared in the document. Because any given document will only contain a few words out of the many words in the dictionary, words that do not appear in the vectorization are represented as implicitly zero as a space saving measure.\n",
    "\n",
    "We can convert our entire original corpus to a list of vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(txt) for txt in processed_corpus]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have vectorized our corpus we can begin to transform it using a **(Gensim) model**, which is used as an abstract term referring to a transformation from one document representation to another. In Gensim, documents are represented as vectors so a model can be thought of as a transformation between two vector spaces. The model learns the details of this transformation during training, when it reads the training corpus.\r\n",
    "\r\n",
    "One simple example of a model is TF-IDF. The TF-IDF model transforms vectors from the BOW representation to a vector space where the frequency counts are weighted according to the relative rarity of each word in the corpus.\r\n",
    "\r\n",
    "Here’s a simple example. Let’s initialize the TF-IDF model, training it on our corpus and transforming the string 'system minors'. As shown below, the TF-IDF model again returns a list of tuples, where the first entry is the token ID and the second entry is the TF-IDF weighting. Note that the ID corresponding to 'system' (which occurred 4 times in the original corpus) has been weighted lower than the ID corresponding to 'minors' (which only occurred twice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.5898341626740045), (11, 0.8075244024440723)]\n"
     ]
    }
   ],
   "source": [
    "tfidf = gensim.models.TfidfModel(bow_corpus)\n",
    "\n",
    "words = \"system minors\".lower().split()\n",
    "print(tfidf[dictionary.doc2bow(words)]) # transforming the \"system minors\" string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have created the model, we can do all sorts of cool stuff. For example, to transform the whole corpus via TF-IDF and index it, in preparation for operations called 'similarity queries':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.similarities.docsim.SparseMatrixSimilarity'>\n",
      "[(0, 0.0), (1, 0.32448703), (2, 0.41707572), (3, 0.7184812), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "index = gensim.similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12)\n",
    "print(type(index))\n",
    "\n",
    "query_document = 'system engineering'.split()\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "sims = index[tfidf[query_bow]] \n",
    "print(list(enumerate(sims))) # querying the similarity of 'query_document' against every document in the corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret the result above, the third document has a similarity score of 0.718=72%, while the second document has a similarity score of 42% etc. We can make this slightly more readable by sorting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.7184812\n",
      "2 0.41707572\n",
      "1 0.32448703\n",
      "0 0.0\n",
      "4 0.0\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 0.0\n"
     ]
    }
   ],
   "source": [
    "for doc_num, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(doc_num, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Corpus Streaming and Vector Space\n",
    "\n",
    "Note that all the corpora above reside fully in memory, as a plain Python list. When the dataset is large, storing all of them in RAM won’t do. Instead, let’s assume the documents are stored in a file on disk, one document per line. Gensim only requires that a corpus be able to return one document vector at a time. To construct the dictionary without loading all texts into memory, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1455 unique tokens: ['by', 'i', 'creatures', 'desire', 'fairest']...)\n",
      "'breath'\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(line.lower().split() for line in open('Shakespeare sonnets.txt'))\n",
    "\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in mini_stoplist if stopword in dictionary.token2id] # generating token IDs to remove stopwords \n",
    " \n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "dictionary.filter_tokens(stop_ids + once_ids) # generating token IDs to remove words that appear only once   \n",
    "\n",
    "dictionary.compactify()  # removing gaps in ID sequence after words that were removed by assigning new word ids to all words.\n",
    "print(dictionary) # this is a gensim.corpora.dictionary.Dictionary object\n",
    "pprint.pprint(dictionary[1046])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have a corpus in vector format, there exist several file formats for serializing a vector space corpus to disks. Gensim implements them via the streaming corpus interface mentioned earlier: texts are read from disk in a lazy fashion, one document at a time, without the whole corpus being read into the main memory at once. One of the more notable file formats is the 'Market Matrix format'. To save a corpus in this format, let's use an example below after creating a toy corpus as a plain Python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_corpus = [[(1, 5),(2, 4), (3, 5)], []]  # make one document empty, just for the heck of it\n",
    "gensim.corpora.MmCorpus.serialize('new_corpus.mm', toy_corpus) # mm stands for 'Marktet Matrix format'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other formats for sure (c.f. the official Gensim documentation). And each format has associated methods interacting under that format.\r\n",
    "\r\n",
    "Corpus objects are streams HERE, so typically we won’t be able to print them directly. To load the data using the 'Market Matrix format', here is what we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(2 documents, 4 features, 3 non-zero entries)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(1, 5.0), (2, 4.0), (3, 5.0)], []]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_corpus = gensim.corpora.MmCorpus('new_corpus.mm')\n",
    "print(loaded_corpus)\n",
    "list(loaded_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim also contains efficient utility functions to help converting from/to numpy matrices (pay attention to the way matrices are stored below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.matutils.Dense2Corpus object at 0x0000028636F18E08>\n",
      "[[4 3]\n",
      " [8 2]\n",
      " [8 1]\n",
      " [8 5]\n",
      " [6 8]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(0, 4.0), (1, 8.0), (2, 8.0), (3, 8.0), (4, 6.0)],\n",
       " [(0, 3.0), (1, 2.0), (2, 1.0), (3, 5.0), (4, 8.0)]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_matrix = np.random.randint(10, size=[5, 2])  # random matrix as an example\n",
    "gensim_corpus = gensim.matutils.Dense2Corpus(numpy_matrix)\n",
    "print(gensim_corpus)\n",
    "print(numpy_matrix)\n",
    "list(gensim_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Topics and Transformations\n",
    "\n",
    "In this section we examine common ways of transforming the text documents. Specifically, we want to bring out hidden structure in the corpus, discover relationships between words and use them to describe the documents in a new and  more semantic way. In addition, we want to make the document representation more compact. This both improves efficiency (new representation consumes less resources) as well as efficacy (noise-reduction).\n",
    "\n",
    "We will use the wine review dataset again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>taster_twitter_handle</th>\n",
       "      <th>title</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Aromas include tropical fruit, broom, brimston...</td>\n",
       "      <td>Vulkà Bianco</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sicily &amp; Sardinia</td>\n",
       "      <td>Etna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>@kerinokeefe</td>\n",
       "      <td>Nicosia 2013 Vulkà Bianco  (Etna)</td>\n",
       "      <td>White Blend</td>\n",
       "      <td>Nicosia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "      <td>Avidagos</td>\n",
       "      <td>87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Douro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>@vossroger</td>\n",
       "      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n",
       "      <td>Portuguese Red</td>\n",
       "      <td>Quinta dos Avidagos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>@paulgwine</td>\n",
       "      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n",
       "      <td>Pinot Gris</td>\n",
       "      <td>Rainstorm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    country                                        description   designation  \\\n",
       "0     Italy  Aromas include tropical fruit, broom, brimston...  Vulkà Bianco   \n",
       "1  Portugal  This is ripe and fruity, a wine that is smooth...      Avidagos   \n",
       "2        US  Tart and snappy, the flavors of lime flesh and...           NaN   \n",
       "\n",
       "   points  price           province           region_1           region_2  \\\n",
       "0      87    NaN  Sicily & Sardinia               Etna                NaN   \n",
       "1      87   15.0              Douro                NaN                NaN   \n",
       "2      87   14.0             Oregon  Willamette Valley  Willamette Valley   \n",
       "\n",
       "     taster_name taster_twitter_handle  \\\n",
       "0  Kerin O’Keefe          @kerinokeefe   \n",
       "1     Roger Voss            @vossroger   \n",
       "2   Paul Gregutt           @paulgwine    \n",
       "\n",
       "                                           title         variety  \\\n",
       "0              Nicosia 2013 Vulkà Bianco  (Etna)     White Blend   \n",
       "1  Quinta dos Avidagos 2011 Avidagos Red (Douro)  Portuguese Red   \n",
       "2  Rainstorm 2013 Pinot Gris (Willamette Valley)      Pinot Gris   \n",
       "\n",
       "                winery  \n",
       "0              Nicosia  \n",
       "1  Quinta dos Avidagos  \n",
       "2            Rainstorm  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename=\"winemag-data-130k-v2.csv\"\n",
    "df = pd.read_csv(filename, index_col=0) # we don't read in row name (index) as a separated column\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get descriptions on all the reviews for only Chardonnay and Cabernet Suavignon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5113176 words in the combination of all review.\n"
     ]
    }
   ],
   "source": [
    "df1=df[df['variety'].isin(['Chardonnay', 'Cabernet Sauvignon'])]\n",
    "both_reviews=\" \".join(review for review in df1.description)\n",
    "print (\"There are {} words in the combination of all review.\".format(len(both_reviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['soft', 'supple', 'plum', 'envelopes', 'oaky', 'structure', 'cabernet', 'supported', '15', 'merlot'], ['coffee', 'chocolate', 'complete', 'picture', 'finishing', 'strong', 'end', 'resulting', 'value-priced', 'wine', 'attractive', 'flavor', 'immediate', 'accessibility'], ['slightly', 'reduced', 'wine', 'offers', 'chalky', 'tannic', 'backbone', 'juicy', 'explosion', 'rich', 'black', 'cherry', 'whole', 'accented', 'throughout', 'firm', 'oak', 'cigar', 'box']]\n",
      "A particular dictionary element:  lifted\n"
     ]
    }
   ],
   "source": [
    "stopwords_list=list(STOPWORDS)\n",
    "rm_punkt_list=[l for l in string.punctuation]\n",
    "\n",
    "tokenized_text1=sent_tokenize(both_reviews)\n",
    "tokenized_text2=[st.strip() for st in tokenized_text1]\n",
    "\n",
    "tokenized_text3=[]\n",
    "for j in tokenized_text2:\n",
    "    tokens=word_tokenize(j)\n",
    "    tokens2=[l.lower() for l in tokens if (l not in rm_punkt_list) & (l not in stopwords_list)] # removing punctuations as well as stopwords in the list\n",
    "    tokenized_text3.append(tokens2)\n",
    "    \n",
    "print(tokenized_text3[0:3])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(tokenized_text3)\n",
    "print('A particular dictionary element: ', dictionary[405])\n",
    "corpus = [dictionary.doc2bow(t) for t in tokenized_text3] # this is a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transformations can be done. Gensim implements several popular vector space model algorithms:\r\n",
    "\r\n",
    "   - The basic BOW TF-IDF model.\r\n",
    "   - Latent symantic indexing model (LSI)\r\n",
    "   - Random projection model (RP)\r\n",
    "   - Latent Dirichlet allocation model (LDA)\r\n",
    "\r\n",
    "We are familiar with LSI and LDA models. Other methods can be learned through reading the official documentation. We won't elaborate on the detail here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Gensim Similarity Queries\n",
    "\n",
    "In previous sections, we covered what it means to create a corpus in the vector space model and how to transform it between different vector spaces in Gensim. A common reason for such a charade is that we want to determine similarity between pairs of documents, or the similarity between a specific document and a set of other documents (such as a user query vs. indexed documents).\n",
    "\n",
    "To show how this can be done in Gensim, let us consider the same corpus as in the the first section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]\n",
    "\n",
    "stoplist = set('for a of the and to in'.split()) # removing common words and tokenize\n",
    "texts = [\n",
    "    [word for word in document.lower().split() if word not in stoplist]\n",
    "    for document in documents\n",
    "]\n",
    "\n",
    "frequency = defaultdict(int) # removing words that appear only once\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "tokenized_texts = [\n",
    "    [token for token in text if frequency[token] > 1]\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(tokenized_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first use LSI to transform the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=2\n",
    "lsi = gensim.models.LsiModel(corpus, id2word=dictionary, num_topics=K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial, there are only two things we need to know about LSI. First, it’s just another transformation: it transforms vectors from one space to another. Second, the benefit of LSI is that enables identifying patterns and relationships between terms (in our case, words in a document) and topics. Our LSI space is K-dimensional (_num\\_topics_ = $K$) so there are $K$ topics in this example. \r\n",
    "\r\n",
    "Now suppose a user typed in the query \"Shall I compare thee to a summer's day. Thou art more lovely and more temperate. Computer human INTERACTION!!!\". We would like to sort our corpus documents in decreasing order of relevance to this query. Unlike modern search engines, here we only concentrate on a single aspect of possible similarities: that is, on apparent semantic relatedness of their texts (words). We will use the cosine similarity measure. The cosine similarity measure looks at the angle of two vectors. Remember the cosine function in mathematics looks at the angle between two vectors. So if $a, b$ are 2 vectors, with the angle in between them to be defined as $\\theta$, then we can define $\\cos(\\theta)=\\frac{a \\cdot b}{||a||||b||}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, -0.4618210045327156), (1, -0.07002766527899836)]\n"
     ]
    }
   ],
   "source": [
    "doc=\"Shall I compare thee to a summer's day. Thou art more lovely and more temperate. Computer human INTERACTION!!!\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow]  # convert the query to LSI space\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for similarity queries, we need to enter all documents which we want to compare against subsequent queries. In our case, they are the same documents used for training LSI, converted to 2-D LSA space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = gensim.similarities.MatrixSimilarity(lsi[corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine measure returns similarities in the range \\[-1, 1\\] (the greater, the more similar). To obtain similarities of our query document against the nine indexed documents, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.998093), (1, 0.93748635), (2, 0.9984453)] \n",
      "\n",
      "0.9984453 (similarity score):  The EPS user interface management system\n",
      "0.998093 (similarity score):  Human machine interface for lab abc computer applications\n",
      "0.9865886 (similarity score):  System and human system engineering testing of EPS\n",
      "0.93748635 (similarity score):  A survey of user opinion of computer system response time\n",
      "0.90755945 (similarity score):  Relation of user perceived response time to error measurement\n",
      "0.050041765 (similarity score):  Graph minors A survey\n",
      "-0.09879464 (similarity score):  Graph minors IV Widths of trees and well quasi ordering\n",
      "-0.10639259 (similarity score):  The intersection graph of paths in trees\n",
      "-0.12416792 (similarity score):  The generation of random binary unordered trees\n"
     ]
    }
   ],
   "source": [
    "sims = index[vec_lsi]  # performing a similarity query against the corpus\n",
    "print(list(enumerate(sims))[0:3], \"\\n\") # sims is a list of tuples\n",
    "\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "for doc_position, doc_score in sims:\n",
    "    print(doc_score, \"(similarity score): \", documents[doc_position]) # tokenized_text2 works as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing to note here is that documents \"The EPS user interface management system\" and \"Relation of user perceived response time to error measurement\" would never be returned by a standard boolean full-text search, because they do not share any common words with \"Human computer interaction\". However, after applying LSI, we can observe that both of them received quite high similarity scores, which corresponds better to our intuition of them sharing a 'computer-human' related topic with the query. In fact, this semantic generalization is the reason why we apply transformations and do topic modelling in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References: \r\n",
    "\r\n",
    "   - https://machinelearningmastery.com/what-are-word-embeddings/\r\n",
    "   - https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\r\n",
    "   - https://radimrehurek.com/gensim/auto_examples/index.html#documentation\r\n",
    "   - https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html\r\n",
    "   - https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html \r\n",
    "   - https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html\r\n",
    "   - https://math.nist.gov/MatrixMarket/formats.html\r\n",
    "   - https://www.coursera.org/lecture/probabilistic-models-in-nlp/continuous-bag-of-words-model-hW72r\r\n",
    "   - https://www.kaggle.com/jannesklaas/17-nlp-and-word-embeddings\r\n",
    "   - https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/ \r\n",
    "   - https://www.tensorflow.org/tutorials/text/word2vec\r\n",
    "   - http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\r\n",
    "   - https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/ \r\n",
    "   - https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4 \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}