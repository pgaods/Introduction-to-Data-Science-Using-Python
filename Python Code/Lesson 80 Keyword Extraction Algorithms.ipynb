{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mini-lecture, we study keyword extraction algorithms (KE). The foundational problem can be summarized as follows: given a paragraph of texts, can we write an algorithm that picks out all the keywords from a content perspective? Luckily, Python has a few built-in algorithm that tackles this type of problem.\r\n",
    "\r\n",
    "There are many applications of keyword extractions. For example, when we are doing online reading, the keyword extraction process not only separates the articles but also helps in saving time on social media platforms. We can take the decision to read the post and comments based on their keywords.\r\n",
    "We can check whether your article belongs to a current trend or not, or our article will trend or not. \r\n",
    "\r\n",
    "There is more than one KE algorithm. We will have a quick review of what they do and provide code examples. The syntaxes of these algorithms are pretty easy to implement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -mart-open (c:\\users\\gao\\.conda\\envs\\gao_uat\\lib\\site-packages)\n",
      "ERROR: Invalid requirement: '+'\n",
      "WARNING: Ignoring invalid distribution -mart-open (c:\\users\\gao\\.conda\\envs\\gao_uat\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mart-open (c:\\users\\gao\\.conda\\envs\\gao_uat\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mart-open (c:\\users\\gao\\.conda\\envs\\gao_uat\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\gao\\.conda\\envs\\gao_uat\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#!pip install rake-nltk\r\n",
    "#!pip install yake\r\n",
    "#!pip install -e git + https://github.com/smirnov-am/pytopicrank.git#egg=pytopicrank \r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import nltk\r\n",
    "import gensim\r\n",
    "import yake\r\n",
    "import os\r\n",
    "\r\n",
    "from rake_nltk import Rake\r\n",
    "from gensim.summarization import keywords # this module only pertains to 3.8.3, and Gensim version above 4.0.1 will not contain the 'summarization' submodule\r\n",
    "from gensim.summarization.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "print(gensim.__version__) # the gensim.summarization package only exists for 3.8.3 and below. The package was deprecated after the release of gensim 4.0 versions or above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path=\"C:\\\\Users\\\\gao\\\\GAO_Jupyter_Notebook\\\\Datasets\"\r\n",
    "path=\"C:\\\\Users\\\\GAO\\\\python workspace\\\\GAO_Jupyter_Notebook\\\\Datasets\"\r\n",
    "os.chdir(path)\r\n",
    "\r\n",
    "#path=\"C:\\\\Users\\\\pgao\\\\Documents\\\\PGZ Documents\\\\Programming Workshop\\\\PYTHON\\\\Open Courses on Python\\\\Udemy Course on Python\\Introduction to Data Science Using Python\\\\datasets\"\r\n",
    "#os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\GAO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GAO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\r\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Yet Another Keyword Extractor (Yake!)\r\n",
    "\r\n",
    "The YAKE! algorithm was proposed in 2020 and was studied in comparison with a bunch of similar algorithms, including statistical methods such as **TF-IDF**, **KP-Miner**, **RAKE**, graph-based methods such as **PageRank** (still used by Google to rank webpages), **TextRank**, **SingleRank**, **ExpandRank**, **TopicRank**, **TopicalPageRank**, **PositionRank**, and supervised methods such as **KEA**. Here are the key features of Yake!: \r\n",
    "\r\n",
    "   1. Unsupervised approach: the algorithm is a light-weight unsupervised automatic keyword extraction algorithm which builds upon local text statistical features extracted from single documents; i.e., it does not require any training corpus.\r\n",
    "   2. Corpus-independent: the algorithm a solution which can retrieve keywords from a single document only, without the need to rely on external document collection statistics as IDF does; i.e., it can be applied to any text.\r\n",
    "   3. Domain and language-independent: YAKE! works with domains and languages for which there are no ready keyword extraction systems, as it neither requires a training corpus nor depends on sophisticated external sources (such as WordNet or Wikipedia) or linguistic tools (such as PoS taggers) other than a static list of stopwords.\r\n",
    "   4. Interior stopwords: YAKE! can retrieve keywords containing interior stopwords (e.g., “game of Thrones”) with higher precision than the state-of-the-art methods.\r\n",
    "   5. Scale: YAKE! scales to any document length linearly in the number of candidate terms identified.\r\n",
    "   6. Term frequency-free: meaning that no conditions are set with respect to the minimum frequency or sentence frequency that a candidate keyword must have. Therefore, based on the features used, a keyword may be considered significant or insignificant with either one occurrence or with multiple occurrences.\r\n",
    "   7. Open-source: finally, we make available a demo [yake.inesctec.pt] [9] and an app on Google Play, as well as an API [yake.inesctec.pt/api] and a python package [github.com/LIAAD/yake], so that the scientific community can test our approach and evaluate it in the future in subsequent studies.\r\n",
    "\r\n",
    "The algorithm has five main steps: (1) text pre-processing and candidate term identification; (2) feature extraction; (3) computing term score; (4) n-gram generation and computing candidate keyword score; and (5) data deduplication and ranking. Details of the algorithm can be found in the original YAKE! paper. The 'Yake' package can help us do the extraction. The output will not only write out the keywords but also a score. The lower the score, the more relevant the keyword is.\r\n",
    "\r\n",
    "The 'yake' Python package is fairly new and is still being developed, so the documentation is still scarce compared to other algorithms. Nonetheless, the package provides a very convenient way of summarizing keywords in text documents. Let's see a concrete example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We are currently trying to understand the Vizient Q&A readmission logic. In the CDB online tool-report templates, we see that there is a set of readmission report\n",
      "templates available for members to pull. We want to focus on specific service lines (cardiology in our case) and understand our poor performance. Our z-score is \n",
      "currently at 2.5, which is already worse than 95 percent of our cohorts. Can we talk to the experts in the analytics team and help us understand this?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\r\n",
    "We are currently trying to understand the Vizient Q&A readmission logic. In the CDB online tool-report templates, we see that there is a set of readmission report\r\n",
    "templates available for members to pull. We want to focus on specific service lines (cardiology in our case) and understand our poor performance. Our z-score is \r\n",
    "currently at 2.5, which is already worse than 95 percent of our cohorts. Can we talk to the experts in the analytics team and help us understand this?\r\n",
    "\"\"\"\r\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('readmission logic', 0.05571858509133558)\n",
      "('Vizient', 0.08320359766886957)\n",
      "('CDB online', 0.1266728539959154)\n",
      "('logic', 0.1501643180173009)\n",
      "('readmission', 0.1757337468250936)\n",
      "('understand', 0.18127425329060223)\n",
      "('readmission report', 0.1974390035217166)\n",
      "('templates', 0.2358520547051198)\n",
      "('CDB', 0.23965767344532257)\n",
      "('online tool-report', 0.2821896149816428)\n",
      "('tool-report templates', 0.2841770066167947)\n",
      "('report templates', 0.2841770066167947)\n",
      "('service lines', 0.36181549152450343)\n",
      "('poor performance', 0.36181549152450343)\n",
      "('pull', 0.38029995855635135)\n",
      "('specific service', 0.4636804578837865)\n",
      "('online', 0.46913127445820996)\n",
      "('tool-report', 0.46913127445820996)\n",
      "('set', 0.46913127445820996)\n",
      "('report', 0.46913127445820996)\n",
      "('members', 0.46913127445820996)\n",
      "('lines', 0.4720441592731822)\n",
      "('cardiology', 0.4720441592731822)\n",
      "('case', 0.4720441592731822)\n",
      "('performance', 0.4720441592731822)\n"
     ]
    }
   ],
   "source": [
    "kw_extractor = yake.KeywordExtractor()\r\n",
    "\r\n",
    "language = \"en\" # YAKE! can be applied to non-English languages as well\r\n",
    "max_ngram_size = 2 # picking the maximum size of n-gram, where n=3 in this case\r\n",
    "deduplication_threshold = 0.9\r\n",
    "numOfKeywords = 25 # number of keywords\r\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\r\n",
    "kws= custom_kw_extractor.extract_keywords(text)\r\n",
    "for kw in kws:\r\n",
    "    print(kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. RAKE (Rapid Automatic Keyword Extraction)\r\n",
    "\r\n",
    "RAKE is a domain independent keyword extraction algorithm which tries to determine key phrases in a body of text by analyzing the frequency of word appearance and its co-occurance with other words in the text. The algorithm requires no training, the only input is a list of stop words for a given language, and a tokenizer that splits the text into sentences and sentences into words. Details can be found in Berry and Kogan (2010). The package needs to be used with NLTK in conjunction (mainly to use its stopwords list). \r\n",
    "\r\n",
    "RAKE is based on the observations that keywords frequently contain multiple words with standard punctuation or stopwords. Words that are considered to carry a meaning related to the text are described as the **content bearing** and are called as **content words**. The input parameters for the RAKE Algorithm comprise a list of stopwords as well as a set of phrase delimiters and word delimiters. The algorithm roughly goes like this: firstly the document text is split into an array of words by the specific word delimiters, and secondly, the array is again split into a sequence of contiguous words at phrase delimiters and stopword positions. Finally, the words that lie in the same sequence are assigned the same position in the text and together are considered as a candidate key. After identifying all the candidate keywords from the text data, a graph of word co-occurrence is generated which calculates the score for each candidate keyword and defined as the **member word score**. With the help of this graph, we evaluate several metrics for calculating word scores, based on the degree and frequency of the vertices in the graph. The major metrics are as follow:\r\n",
    "\r\n",
    "   - word frequencies: favors words that occur frequently regardless of the number of words with which they co-occurred\r\n",
    "   - word degrees: favors words that occur often and in longer candidates\r\n",
    "   - ratio of degree to frequency: favors the words that predominately occur in longer candidate keywords\r\n",
    "\r\n",
    "The final score for each candidate keyword is calculated as the sum of its member word scores. After the candidate keyword score is calculated, the top $T$ candidate keywords are selected from the document. The **T value** is one-third the number of words in the graph. So RAKE is not dependent on any word embeddings; it's based on frequencies and graphs. \r\n",
    "\r\n",
    "The details of the algorithm can be found from the following link:\r\n",
    "\r\n",
    "   - https://medium.datadriveninvestor.com/rake-rapid-automatic-keyword-extraction-algorithm-f4ec17b2886c. \r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Rake() # using stopwords fro English from NLTK, and all punctuation characters by default\r\n",
    "r.extract_keywords_from_text(text) # extracting given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spacy', 'cython', 'spacy functionally', 'advantages', 'source software library', 'main developers', 'open', 'software company explosion', 'contrast', 'founders', 'written', 'published', 'advanced natural language processing', 'nltk', 'users often encounter different types', 'ines montani', 'general', 'gensim package mainly handles word embeddings', 'programming languages python', 'matthew honnibal', 'whereas spacy combines', 'powerful nlp tool', 'data scientists enjoy using besides gensim', 'mit license', 'word similarities', 'package', 'library', 'basic', 'tricky business', 'technical glitches', 'installation', 'gensim'}\n"
     ]
    }
   ],
   "source": [
    "print(set(r.get_ranked_phrases()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(34.333333333333336, 'data scientists enjoy using besides gensim'),\n (9.0, 'powerful nlp tool'),\n (7.75, 'whereas spacy combines'),\n (6.0, 'word similarities'),\n (9.0, 'software company explosion'),\n (8.0, 'source software library'),\n (29.833333333333332, 'gensim package mainly handles word embeddings'),\n (9.0, 'programming languages python'),\n (16.0, 'advanced natural language processing'),\n (25.0, 'users often encounter different types')]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rakescores=set(r.get_ranked_phrases_with_scores()) # getting keyword phrases ranked highest to lowerst with scores\r\n",
    "[i for i in rakescores if i[0]>=5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. TextRank \r\n",
    "\r\n",
    "The TextRank algorithm can be implemented in the Gensim 3.8.3 version package. Notice that this functionatliy was deprecated in later versions such as 4.0 versions above. Here we focus on the lower version number. The foundation of TextRank algorithm is the PageRank algorithm, which Google uses to rank page relevance of search results. In TextRank, the only difference is that we consider sentences instead of pages.\r\n",
    "\r\n",
    "TextRank is a general purpose, graph-based ranking algorithm for NLP. Graph-based ranking algorithms are a way for deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph. When one vertex links to another one, it is basically casting a vote for that vertex. The higher the number of votes cast for a vertex, the higher the importance of that vertex. Thus, we have to build a graph that represents the text, interconnects words or other text entities with meaningful relations. TextRank includes two NLP tasks:\r\n",
    "   \r\n",
    "   - Keyword extraction task\r\n",
    "   - Sentence extraction task\r\n",
    "\r\n",
    "TextRank is very well suited for applications involving entire sentences, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text. To apply TextRank, we first build a graph associated with the text, where the graph vertices are representative for the units to be ranked. The goal is to rank entire sentences, therefore, a vertex is added to the graph for each sentence in the text.\r\n",
    "\r\n",
    "Let's apply TextRank to the existing text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('languages', 0.3103891302404008), ('software', 0.24152957209043588), ('word', 0.2407147735187126), ('spacy', 0.2082656110080652), ('scientists', 0.20826561100806484), ('different', 0.20826561100806482), ('nlp', 0.2082656110080647), ('company', 0.16913153914645165)]\n"
     ]
    }
   ],
   "source": [
    "print(keywords(text, scores=True, lemmatize=True)) # using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also do text summarization using the Gensim package as well. The idea is the same as extracting keywords. For TextRank, the selected text fragments to use in the graph construction can be phrases, sentences, or paragraphs. Currently, many successful systems adopt the sentences considering the tradeoff between content richness and grammar correctness. According to these approach the most important sentences are the most connected ones in the graph and are used for building a final summary. The API gensim.summarization.summarizer.summarize() has a few tuning parameters. The _word\\_count_ argument specifies the maximum amount of words we want in the summary. The _ratio_ argument specifies what fraction of sentences in the original text should be returned as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'spaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython.'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(text, word_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a more complicated example. Using the Bible dataset, let's try to summarize biblical texts using TextRank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible = pd.read_csv('t_kjv.csv')\r\n",
    "df_aux = pd.read_csv(\"BibleBooks.csv\") # additional information related to the Bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible.loc[bible['b'] <= 39, 'Testament'] = 'OT'\r\n",
    "bible.loc[bible['b'] > 39, 'Testament'] = 'NT'\r\n",
    "\r\n",
    "df_enriched = df_aux.drop(['Tanakh','New Jerusalem Version'], axis=1)\r\n",
    "df_enriched['King James Version']=df_enriched['King James Version'].replace(np.nan, 0)\r\n",
    "df_enriched['King James Version'] = df_enriched['King James Version'].astype('int')\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Book Index</th>\n      <th>Chapter Index</th>\n      <th>Verse</th>\n      <th>Testament</th>\n      <th>Book</th>\n      <th>Time</th>\n      <th>Period</th>\n      <th>Location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1001001</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>OT</td>\n      <td>Genesis</td>\n      <td>-500</td>\n      <td>Persian</td>\n      <td>Israel</td>\n      <td>in the beginning god created the heaven and th...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1001002</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>OT</td>\n      <td>Genesis</td>\n      <td>-500</td>\n      <td>Persian</td>\n      <td>Israel</td>\n      <td>and the earth was without form, and void; and ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1001003</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>OT</td>\n      <td>Genesis</td>\n      <td>-500</td>\n      <td>Persian</td>\n      <td>Israel</td>\n      <td>and god said, let there be light: and there wa...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1001004</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>OT</td>\n      <td>Genesis</td>\n      <td>-500</td>\n      <td>Persian</td>\n      <td>Israel</td>\n      <td>and god saw the light, that it was good: and g...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1001005</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>OT</td>\n      <td>Genesis</td>\n      <td>-500</td>\n      <td>Persian</td>\n      <td>Israel</td>\n      <td>and god called the light day, and the darkness...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "        id  Book Index  Chapter Index  Verse Testament     Book  Time  \\\n0  1001001           1              1      1        OT  Genesis  -500   \n1  1001002           1              1      2        OT  Genesis  -500   \n2  1001003           1              1      3        OT  Genesis  -500   \n3  1001004           1              1      4        OT  Genesis  -500   \n4  1001005           1              1      5        OT  Genesis  -500   \n\n    Period Location                                               text  \n0  Persian   Israel  in the beginning god created the heaven and th...  \n1  Persian   Israel  and the earth was without form, and void; and ...  \n2  Persian   Israel  and god said, let there be light: and there wa...  \n3  Persian   Israel  and god saw the light, that it was good: and g...  \n4  Persian   Israel  and god called the light day, and the darkness...  "
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = bible.merge(df_enriched, left_on='b', right_on='King James Version')\r\n",
    "df.rename(columns={'b': 'Book Index', 'c': 'Chapter Index', 'v': 'Verse', 't': 'Text'}, inplace=True)\r\n",
    "df.drop(['King James Version'], axis=1, inplace=True)\r\n",
    "df['text'] = df['Text'].str.lower()\r\n",
    "df.drop(['Text'], axis=1, inplace=True)\r\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "paul, a servant of jesus christ, called to be an apostle, separated unto the gospel of god, (which he had promised afore by his prophets in the holy scriptures,) concerning his son jesus christ our lord, which was made of the seed of david according to the flesh; and declared to be the son of god with power, according to the spirit of holiness, by the resurrection from the dead: by whom we have received grace and apostleship, for obedience to the faith among all nations, for his name: among whom are ye also the called of jesus christ: to all that be in rome, beloved of god, called to be saints: grace to you and peace from god our father, and the lord jesus christ.\n",
      "but after thy hardness and impenitent heart treasurest up unto thyself wrath against the day of wrath and revelation of the righteous judgment of god; who will render to every man according to his deeds: to them who by patient continuance in well doing seek for glory and honour and immortality, eternal life: but unto them that are contentious, and do not obey the truth, but obey unrighteousness, indignation and wrath, tribulation and anguish, upon every soul of man that doeth evil, of the jew first, and also of the gentile; but glory, honour, and peace, to every man that worketh good, to the jew first, and also to the gentile: for there is no respect of persons with god.\n",
      "for when the gentiles, which have not the law, do by nature the things contained in the law, these, having not the law, are a law unto themselves: which shew the work of the law written in their hearts, their conscience also bearing witness, and their thoughts the mean while accusing or else excusing one another;) in the day when god shall judge the secrets of men by jesus christ according to my gospel.\n",
      "therefore it is of faith, that it might be by grace; to the end the promise might be sure to all the seed; not to that only which is of the law, but to that also which is of the faith of abraham; who is the father of us all, (as it is written, i have made thee a father of many nations,) before him whom he believed, even god, who quickeneth the dead, and calleth those things which be not as though they were.\n",
      "for if through the offence of one many be dead, much more the grace of god, and the gift by grace, which is by one man, jesus christ, hath abounded unto many.\n",
      "for i am persuaded, that neither death, nor life, nor angels, nor principalities, nor powers, nor things present, nor things to come, nor height, nor depth, nor any other creature, shall be able to separate us from the love of god, which is in christ jesus our lord.\n"
     ]
    }
   ],
   "source": [
    "Romans=df.loc[df['Book']=='Romans']\r\n",
    "l=[v for v in Romans['text']]\r\n",
    "ls=' '.join((str(n) for n in l))\r\n",
    "print(summarize(ls, word_count=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\r\n",
    "\r\n",
    "   - https://towardsdatascience.com/keyword-extraction-process-in-python-with-natural-language-processing-nlp-d769a9069d5c \r\n",
    "   - https://towardsdatascience.com/keyword-extraction-python-tf-idf-textrank-topicrank-yake-bert-7405d51cd839\r\n",
    "   - https://medium.com/@shivangisareen/text-summarisation-with-gensim-textrank-46bbb3401289\r\n",
    "   - https://datascience.stackexchange.com/questions/23969/sentence-similarity-prediction\r\n",
    "   - https://www.sciencedirect.com/science/article/abs/pii/S0020025519308588?via%3Dihub\r\n",
    "   - https://github.com/LIAAD/yake\r\n",
    "   - https://pypi.org/project/rake-nltk/ \r\n",
    "   - https://medium.datadriveninvestor.com/rake-rapid-automatic-keyword-extraction-algorithm-f4ec17b2886c \r\n",
    "   - https://www.bibsonomy.org/bibtex/1f2c1ce382d62625a1c3aeca81e96c4b4/lopusz_kdd\r\n",
    "   - https://radimrehurek.com/gensim_3.8.3/auto_examples/tutorials/run_summarization.html\r\n",
    "   - Michael W. Berry and Jacob Kogan (2010). Text Mining: Applications and Theory. John Wiley & Sons.\r\n",
    "   - https://aclanthology.org/I13-1062.pdf "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "83242e1a9ec8b643d57fa32edcbc4e662e00a392e679248885719c5ceb6ac4dd"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}