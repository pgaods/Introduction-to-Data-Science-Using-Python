{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we focus on Python's ability for natural language processing (NLP). Our goal is to understand the basics of text mining and then perform a Naive Bayes classifier on the text to build a model. The focus here are two-folds: 1) We need to learn how to 'featurize' the text, i.e. how we can extract useful information from a text document, and 2) we need to learn how to build a machine learning model based on the underlying text data. This lecture only serves as a crash course for NLP. Further details can be found in more advanced books. There are three good sources of natural language processsing, all of which are listed in the reference sections:\n",
    "\n",
    "1. Manning, Raghavan and Schuetze (2008), Introduction to Information Retrieval\n",
    "2. Bird, Klein and Loper (2009), Natural Language Processing with Python-Analyzing Text with the Natural Language Toolkit\n",
    "3. Jurafsky and Martin (2017), Speech and Language Processing\n",
    "\n",
    "We first go over some important concepts in NLP. Then we gradually discuss techniques in Python. From a programmer's point of view. NLP is challenging in 2 aspects: 1) information retrieval (how can we extract all the useful information from the unstructered texts?) 2) machine learning altorithm (how can we leverage statistics and machine learning algorithms to study patterns?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we all know that texts are built on strings, which can later form words, and sentences, and articles, and more. To organize all these concepts, we start our discussion with the concept of a corpus. A **corpus** is simply defined to be a large collection of texts. It is a body of written or spoken material upon which a linguistic analysis is based. The plural form of corpus is corpora. The point of defining a corpus is that we have a scope of the text data we can extract information from and perform linguistic or statistical analysis upon. \n",
    "\n",
    "The NLP package itself contains a sea of corpus. The nltk.download() command will help us navigate through varieties of corpora to download. Let's use one example here. Below displays some statistics for each text in a corpus: average word length, average sentence length, and the number of times each vocabulary item appears in the text on average (our lexical diversity score):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.download() # uncomment this if you have installed all the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.609909212324673 24.822884416924666 26.201933551198255 austen-emma.txt\n",
      "4.749793727271801 26.19989324793168 16.82450728363325 austen-persuasion.txt\n",
      "4.753785952421314 28.32086417283457 22.11088552241137 austen-sense.txt\n",
      "4.286881563819072 33.57319868451649 79.16143181640166 bible-kjv.txt\n",
      "4.567033756284415 19.073059360730593 5.442345276872964 blake-poems.txt\n",
      "4.489300433741879 19.40726510653161 14.10228426395939 bryant-stories.txt\n",
      "4.464641670621737 17.99146110056926 12.163566388710713 burgess-busterbrown.txt\n",
      "4.233216065669891 20.029359953024077 12.940060698027315 carroll-alice.txt\n",
      "4.716173862839705 20.296296296296298 11.637192561487703 chesterton-ball.txt\n",
      "4.724783007796614 22.61245401996847 11.042211957916345 chesterton-brown.txt\n",
      "4.63099417739442 18.496258685195084 10.901401795558355 chesterton-thursday.txt\n",
      "4.4391184023772565 20.59266862170088 24.939386764531786 edgeworth-parents.txt\n",
      "4.76571875515204 25.928919375683467 15.136614241773549 melville-moby_dick.txt\n",
      "4.835734572682675 52.309562398703406 10.73328899235118 milton-paradise.txt\n",
      "4.347539968257655 11.943134535367545 8.520118733509236 shakespeare-caesar.txt\n",
      "4.3597698072805136 12.028332260141662 7.921967769296014 shakespeare-hamlet.txt\n",
      "4.336689714779602 12.134242265338228 6.680138568129331 shakespeare-macbeth.txt\n",
      "4.591950052620365 36.44305882352941 12.438403469322198 whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set([w.lower() for w in gutenberg.words(fileid)]))\n",
    "    print(num_chars/num_words, num_words/num_sents, num_words/num_vocab, fileid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We now have seen a corpus, so we need learn to extract information from the text and convert the information possibly using numerals so that later we can do machine learning on the text. Let's start from the very basic information. Suppose we have a text document and it has two lines of words: 1) blue house, and 2) red house. Here, the simpliest information we can extract is the word count. Each word can be vectorized by a bunch of 0's and 1's. A document represented as a vector of word counts is called a **bag of words**. In this example, we can create a word vector [red,blue,house]. This way, the first line of words 'blue house' can be represented by [red,blue,house]=[0,1,1], whereas the second line of words can be represented by [red,blue,house]=[1,0,1]. Once we vectorize the text, we can define many concepts such as similarity of two line of words (e.g. using the cosine function). The idea here is very similar to what data scientists do in dealing with healthcare administrative data: suppose you have a dataset of inpatient encounters (one line per encounter) and each encounter is represented by a patient having a variety of diagnosis codes (in billing data, we can expect up to 99 diagnosis codes and 99 procedure codes etc.). What people usually do to extract features is to create flags for each diagnosis code and give it a 1 or 0. The point of the text featurization is to extract information and represent them in numeric values so that we can later apply machine learning models easily. \n",
    "\n",
    "In the bag of words model, there are a few metrics that go along easily with word count. For example, **term frequency** (TF) helps us understand the importance of the term in the document. Mathematically, for a generic term t and a document d, TF(t,d)= number of occurrences of term t in document d. Similarly, the **inverse document frequency** (IDF) measures the importance of the term in the corpus. Mathematically, for a specific term t in the corpus with total number of documents equal D, IDF(t,T,D)=log(D/T(t)), where T(t) represents the number of documents that contain the specific term t (T is essentially a frequency). We can also define the **term frequency-inverse document frequent** (TF-IDF) as the product of TF and IDF, which gives us a composite weight for each term in each document. There are different versions of TF. The definitions here we give is the most original one. The online manuscript 'An Introduction to Information Retrieval' has more details on this topic. We will skip the details for now. \n",
    "\n",
    "For now, let's see an example. In the NLTK package, the Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on. This corpus is a convenient resource for studying systematic differences between genres, a kind of linguistic inquiry known as **stylistics**. Let's compare genres in their usage of modal verbs (modal verbs are words that include 'can', 'may', 'must' etc.). The first step is to produce the counts for a particular genre. Next, we will obtain counts for each genre of interest. We'll use NLTK's support for conditional frequency distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction'] \n",
      "\n",
      "Word counts in the 'news' cateogry of the Brown Corpus:\n",
      "can: 94\n",
      "could: 87\n",
      "may: 93\n",
      "might: 38\n",
      "must: 53\n",
      "will: 389\n",
      "\n",
      "\n",
      "Conditional Frequencies for Different Genres:\n",
      "                  can could   may might  must  will \n",
      "           news    94    87    93    38    53   389 \n",
      "       religion    84    59    79    12    54    72 \n",
      "        hobbies   276    59   143    22    84   269 \n",
      "science_fiction    16    49     4    12     8    17 \n",
      "        romance    79   195    11    51    46    49 \n",
      "          humor    17    33     8     8     9    13 \n"
     ]
    }
   ],
   "source": [
    "print(brown.categories(), '\\n') # looking at the categories (genres) in this corpus\n",
    "\n",
    "news_text = brown.words(categories='news')\n",
    "fdist = nltk.FreqDist([w.lower() for w in news_text])\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "print(\"Word counts in the 'news' cateogry of the Brown Corpus:\")\n",
    "for m in modals:\n",
    "    print(m + ':', fdist[m])\n",
    "    \n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (genre, word.lower())\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre))\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "print('\\n')\n",
    "print('Conditional Frequencies for Different Genres:')\n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having word count in our data matrix is useful, but we need more information certainly for bigger projects. In terms of linguistics, there are three important concepts that we often hear from NLP expert. First, given a character sequence and a defined document unit, **tokenization** is the task of chopping it up into pieces, called **tokens**, perhaps at the same time throwing away certain characters, such as punctuation. Here is an example: \n",
    "\n",
    "   * Input: Friends, Romans, Countrymen, lend me your ears!\n",
    "   * Output: ['Friends', 'Romans', 'Countrymen', 'lend', 'me', 'your', 'ears']\n",
    "\n",
    "The above tokenization example merely slices up the sentence and removes punctuations. These tokens are often loosely referred to as **terms or words**, but it is sometimes important to make a type/token distinction. A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. A **type** is the class of all tokens containing the same character sequence. The major question of the tokenization phase is what are the correct tokens to use? In this example, it looks fairly trivial: you chop on whitespace and throw away punctuation characters. This is a starting point, but even for English there are a number of tricky cases. For example, what do you do about the various uses of the apostrophe for possession and contractions? In addition, different languages may require different tokenization strategies. Tokenization is always an interesting yet important topic for NLP practitioners. \n",
    "\n",
    "Second, a **lemma** (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words (headword). In English for example, the word 'run', 'runs', 'ran' and 'running'' are forms of the same **lexeme** (a unit of lexical meaning that exists regardless of the number of inflectional endings it may have or the number of words it may contain), with 'run' as the lemma. The process called **lemmatization** essentially helps us make our data more normalized in some way.\n",
    "In computer linguistic literature, a related concept is **stemming**, which is the process of reducing inflected, or sometimes derived words, to their word stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. In comparison, lemmatization is the algorithmic process of determining the lemma of a word based on its intended meaning. Unlike stemming, lemmatization depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document. As a result, developing efficient lemmatization algorithms is an open area of research. \n",
    "\n",
    "Essentially, stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word. For example, if confronted with the token 'saw', stemming might return just 's', whereas lemmatization would attempt to return either 'see' or 'saw' depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source.\n",
    "\n",
    "The third concept is **part of speech**. In traditional grammar, a part of speech (sometimes abbreviated as PoS or POS) is a category of words (or, more generally, of lexical items) which have similar grammatical properties. Words that are assigned to the same part of speech generally display similar behavior in terms of syntax—they play similar roles within the grammatical structure of sentences—and sometimes in terms of morphology, in that they undergo inflection for similar properties. Commonly listed English parts of speech are noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, and sometimes numeral, article or determiner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hare made some preparations from the theory perspective, now let's examine our dataset. In NLP, the datasets can come in many forms, so each file may require different techniques of data cleaning and feature extraction. Here, the file we are using contains a collection of more than 5 thousand SMS phone messages. The dataset we will be using is from the UCI dataset: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\pgao\\\\Documents\\\\PGZ Documents\\\\Programming Workshop\\\\PYTHON\\\\Open Courses on Python\\\\Udemy Course on Python\\Introduction to Data Science Using Python\\\\datasets\\\\smsspamcollection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    }
   ],
   "source": [
    "data = [line.rstrip() for line in open('SMSSpamCollection')]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's print the first ten messages and number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "\n",
      "\n",
      "1 ham\tOk lar... Joking wif u oni...\n",
      "\n",
      "\n",
      "2 spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n",
      "\n",
      "3 ham\tU dun say so early hor... U c already then say...\n",
      "\n",
      "\n",
      "4 ham\tNah I don't think he goes to usf, he lives around here though\n",
      "\n",
      "\n",
      "5 spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, Â£1.50 to rcv\n",
      "\n",
      "\n",
      "6 ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "\n",
      "\n",
      "7 ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "\n",
      "\n",
      "8 spam\tWINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "\n",
      "\n",
      "9 spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message_no, message in enumerate(data[:10]):\n",
    "    print(message_no, message)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the spacing we can tell that this is a TSV (tab separated values) file, where the first column is a label saying whether the given message is a normal message (commonly known as 'ham') or a spam message (garbage text). The second column is the message itself. Using these labeled 'ham' and 'spam' examples, we'll need to train a machine learning model to learn to discriminate between ham/spam automatically. Then, with a trained model, we'll be able to classify arbitrary unlabeled messages as 'ham' or 'spam'.\n",
    "\n",
    "Dealing with this type of data, the first step is to clean the data and transform the type of our data object into something familiar. Naturally, we think of 'pandas'.Let's try the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('SMSSpamCollection', sep='\\t', names=[\"label\", \"message\"])\n",
    "print(type(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we used the read_csv() and make note of the 'sep' argument. We can also specify the desired column names by passing in a list of 'names' (column names) so that we have a complete 'DataFrame' object. Now let's do some explorative analyses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how balanced is this data (i.e., is the data very sparse?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ham': 4825, 'spam': 747}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(Counter(data.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we continue our analysis we want to start thinking about the features we are going to be using. This goes along with the general idea of **feature engineering**. Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data. In general, the better your domain knowledge on the data, the better your ability to engineer more features from it.\n",
    "\n",
    "Let's start by making a new column to detect how long the text messages are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message lengths description: \n",
      "\n",
      "count    5572.000000\n",
      "mean       80.489950\n",
      "std        59.942907\n",
      "min         2.000000\n",
      "25%        36.000000\n",
      "50%        62.000000\n",
      "75%       122.000000\n",
      "max       910.000000\n",
      "Name: length, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['length'] = data['message'].apply(len)\n",
    "print('message lengths description: \\n')\n",
    "print(data.length.describe()) \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1bf1f7e5ef0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDA5JUicGhySpk/8PvFgBUO51L8UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bf1fd6ab38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['length'].plot(bins=50, kind='hist') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the distribution, we see that there are tons of messages that are ultra long, let's find one of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['length'] == 910]['message'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have some sort of Romeo sending texts! But let's focus back on the idea of trying to see if message length is a distinguishing feature between 'ham' and 'spam'. What the picture belwo tells us is that the message length is possibly a very good feature to detect spams, as spams tend to be characterized with longer word length. In fact, our visualization confirms our conjecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x000001BF204E27B8>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x000001BF2055B860>], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuUAAAEQCAYAAAAXjQrJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuSRJktSYoVySJElqzFAuSZIkNfb/AYcIM/OA9920AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bf1fd6f048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.hist(column='length', by='label', bins=50,figsize=(12,4), color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully engineered one feature so far, which is likely to be very useful when it comes to classification tasks later. We now need to keep up with the good momentum. Nevertheless, we have not dug deep into the text itself. If we can extract more information from the text, then we will have another set of features created for later use of machine learning tasks. There are actually many methods to convert a corpus to a vector format. The simplest is the the bag-of-words approach, where each unique word in a text will be represented by one number. To do so, intuitively speaking, we will need to write a function that will split a message into its individual words and return a list. We'll also remove very common words, ('the', 'a', etc.) and then obtain our features. To implement all these, let's create a function that will process the string in the message column, then we can just use the apply() function in 'pandas' do process all the text in the 'DataFrame object consistently across texts. In this exercise, we will 1) remove all the punctuations. 2) Remove all the **stopwords** (e.g. me, myself, ours, our, yours etc.). \n",
    "\n",
    "First let's get rid of punctuation. We can just take advantage of Python's built-in 'string' library to get a quick list of all the possible punctuation. Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sample message Notice it has punctuation'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mess = 'Sample message! Notice: it has punctuation.'\n",
    "\n",
    "nopunc = [char for char in test_mess if char not in string.punctuation] # checking characters to see if they are in punctuation\n",
    "nopunc = ''.join(nopunc) # joining the characters again to form the string\n",
    "nopunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see how to remove stopwords. We can import a list of English stopwords from NLTK. Below, let's show some stopwords to an idea about what stopwords should like like. There are currently 179 stopwords defined in the package. These need to be removed from our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some Examples of stopwords:\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only'] \n",
      "\n",
      "179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Sample', 'message', 'Notice', 'punctuation']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Some Examples of stopwords:')\n",
    "print(stopwords.words('english')[100:120], '\\n')\n",
    "print(len(stopwords.words('english')))\n",
    "clean_mess = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "clean_mess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put both of these together in a function to apply it to our DataFrame later on: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation.\n",
    "    2. Remove all stopwords.\n",
    "    3. Returns a list of the cleaned text.\n",
    "    \"\"\"\n",
    "    nopunc = [char for char in mess if char not in string.punctuation] # checking characters to see if they are in punctuation\n",
    "    nopunc = ''.join(nopunc) # joining the characcters again to form the string\n",
    "    \n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')] # removing stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the effect of this function, let's apply the function to the first 5 records of our dataset and then compare it with the original message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, jurong, point, crazy, Available, bugis, n...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3        [U, dun, say, early, hor, U, c, already, say]\n",
       "4    [Nah, dont, think, goes, usf, lives, around, t...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['message'].head(5).apply(simple_text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('original message:')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>message2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>[Go, jurong, point, crazy, Available, bugis, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>[Nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n",
       "3   ham  U dun say so early hor... U c already then say...      49   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61   \n",
       "\n",
       "                                            message2  \n",
       "0  [Go, jurong, point, crazy, Available, bugis, n...  \n",
       "1                     [Ok, lar, Joking, wif, u, oni]  \n",
       "2  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...  \n",
       "3      [U, dun, say, early, hor, U, c, already, say]  \n",
       "4  [Nah, dont, think, goes, usf, lives, around, t...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['message2']=data['message'].apply(simple_text_process)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's be more sophisticated. For grammatical reasons, we all know that documents are going to use different forms of a word, such as 'organize', 'organizes', and 'organizing'. Additionally, there are families of derivationally related words with similar meanings, such as 'democracy', 'democratic', and 'democratization'. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
    "\n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form, and from a machine learning standpoint, both stemming and lemmatization helps us with dimensionality reduction. For example, if we see a list of words ['is', 'was', 'been'], we want to reduce it to ['be']. If we have [\"car\", \"cars\", \"cars'\"], we want to collapse it into ['car']. \n",
    "\n",
    "There are many algorithms for stemming and lemmatization. Both stemming and lemmatizationn are constantly used in conjunction with regular expressions, which we will not cover here in detail. NLTK includes several off-the-shelf stemmers, and if you ever need a stemmer you should use one of these in preference to crafting your own using regular expressions, since these handle a wide range of irregular cases. The Porter and Lancaster stemmers follow their own rules for stripping affixes. In contrast, the 'WordNet' lemmatizer only removes affixes if the resulting word is in its dictionary. Below are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Tokens:\n",
      " ['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'lying', 'in', 'ponds', 'distributing', 'swords', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'masses', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n",
      "\n",
      "\n",
      "Using Porter Stemmer:\n",
      " ['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n",
      "\n",
      "\n",
      "Using Lancaster Stemmer:\n",
      " ['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n",
      "\n",
      "\n",
      "Using Lemmatization:\n",
      " ['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "... is no basis for a system of government.  Supreme executive power derives from\n",
    "... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(raw)\n",
    "print('Raw Tokens:\\n', tokens)\n",
    "print('\\n')\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "print('Using Porter Stemmer:\\n', [porter.stem(t) for t in tokens])\n",
    "print('\\n')\n",
    "print('Using Lancaster Stemmer:\\n', [lancaster.stem(t) for t in tokens])\n",
    "print('\\n')\n",
    "print('Using Lemmatization:\\n', [wnl.lemmatize(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most of the time, lemmatization is more superior to stemming as we have discussed previously. For this project, we will pick lemmatization as our primary approach and then we will apply it to the messages in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>message2</th>\n",
       "      <th>message3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>[Go, jurong, point, crazy, Available, bugis, n...</td>\n",
       "      <td>[Go, jurong, point, crazy, Available, bugis, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>[Nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "      <td>[Nah, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>147</td>\n",
       "      <td>[FreeMsg, Hey, darling, 3, weeks, word, back, ...</td>\n",
       "      <td>[FreeMsg, Hey, darling, 3, week, word, back, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>77</td>\n",
       "      <td>[Even, brother, like, speak, treat, like, aids...</td>\n",
       "      <td>[Even, brother, like, speak, treat, like, aid,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>160</td>\n",
       "      <td>[per, request, Melle, Melle, Oru, Minnaminungi...</td>\n",
       "      <td>[per, request, Melle, Melle, Oru, Minnaminungi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>157</td>\n",
       "      <td>[WINNER, valued, network, customer, selected, ...</td>\n",
       "      <td>[WINNER, valued, network, customer, selected, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>154</td>\n",
       "      <td>[mobile, 11, months, U, R, entitled, Update, l...</td>\n",
       "      <td>[mobile, 11, month, U, R, entitled, Update, la...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n",
       "3   ham  U dun say so early hor... U c already then say...      49   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61   \n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...     147   \n",
       "6   ham  Even my brother is not like to speak with me. ...      77   \n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...     160   \n",
       "8  spam  WINNER!! As a valued network customer you have...     157   \n",
       "9  spam  Had your mobile 11 months or more? U R entitle...     154   \n",
       "\n",
       "                                            message2  \\\n",
       "0  [Go, jurong, point, crazy, Available, bugis, n...   \n",
       "1                     [Ok, lar, Joking, wif, u, oni]   \n",
       "2  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...   \n",
       "3      [U, dun, say, early, hor, U, c, already, say]   \n",
       "4  [Nah, dont, think, goes, usf, lives, around, t...   \n",
       "5  [FreeMsg, Hey, darling, 3, weeks, word, back, ...   \n",
       "6  [Even, brother, like, speak, treat, like, aids...   \n",
       "7  [per, request, Melle, Melle, Oru, Minnaminungi...   \n",
       "8  [WINNER, valued, network, customer, selected, ...   \n",
       "9  [mobile, 11, months, U, R, entitled, Update, l...   \n",
       "\n",
       "                                            message3  \n",
       "0  [Go, jurong, point, crazy, Available, bugis, n...  \n",
       "1                     [Ok, lar, Joking, wif, u, oni]  \n",
       "2  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...  \n",
       "3      [U, dun, say, early, hor, U, c, already, say]  \n",
       "4  [Nah, dont, think, go, usf, life, around, though]  \n",
       "5  [FreeMsg, Hey, darling, 3, week, word, back, I...  \n",
       "6  [Even, brother, like, speak, treat, like, aid,...  \n",
       "7  [per, request, Melle, Melle, Oru, Minnaminungi...  \n",
       "8  [WINNER, valued, network, customer, selected, ...  \n",
       "9  [mobile, 11, month, U, R, entitled, Update, la...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_text_process2(mess):\n",
    "        wnl = nltk.WordNetLemmatizer()\n",
    "        return [wnl.lemmatize(t) for t in mess] \n",
    "\n",
    "data['message3']=data['message2'].apply(simple_text_process2)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a more organized and 'cleaner' version of our data, let's perform vectorization. We'll do that in three steps using the bag-of-words model (there are many other ways of course, but here we will only cover this model for illustrative purposes):\n",
    "\n",
    "1. Count how many times does a word occur in each message (term frequency).\n",
    "\n",
    "2. Weigh the counts, so that frequent tokens get lower weight (using inverse document frequency).\n",
    "\n",
    "3. Normalize the vectors to unit length, to abstract from the original text length (using the L2 norm).\n",
    "\n",
    "Let's begin the first step. Notice that each vector will have as many dimensions as there are unique words in the SMS corpus.  We will first use the CountVectorizer() function to convert a collection of text documents to a matrix of token counts. We can imagine this as a 2-dimensional matrix. Where the 1-dimension is the entire vocabulary (1 row per word) and the other dimension are the actual documents, in this case a column per text message. \n",
    "\n",
    "For example:\n",
    "\n",
    "<table border = “1“>\n",
    "<tr>\n",
    "<th></th> <th>Textstring 1</th> <th>Textstring 2</th> <th>...</th> <th>Textstring N</th> \n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word 1 Count</b></td><td>0</td><td>1</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word 2 Count</b></td><td>0</td><td>0</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>...</b></td> <td>1</td><td>2</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word M Count</b></td> <td>0</td><td>1</td><td>...</td><td>1</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Since there are so many messages, we can expect a lot of zero counts for the presence of that word in that document. Because of this, the library will output a sparse matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10943\n"
     ]
    }
   ],
   "source": [
    "bow_transformer = CountVectorizer(analyzer=simple_text_process2).fit(data['message3'])\n",
    "print(len(bow_transformer.vocabulary_)) # printing total number of vocab words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's do this step by step. Let's take one random text message (the 107th message) and get its bag-of-words counts as a vector, putting to use our new bow_transformer() and then see the result from vectorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thanks', 'lot', 'wish', 'birthday', 'Thanks', 'making', 'birthday', 'truly', 'memorable']\n",
      "  (0, 3953)\t2\n",
      "  (0, 5016)\t2\n",
      "  (0, 7668)\t1\n",
      "  (0, 7771)\t1\n",
      "  (0, 7867)\t1\n",
      "  (0, 10207)\t1\n",
      "  (0, 10630)\t1\n",
      "(1, 10943)\n"
     ]
    }
   ],
   "source": [
    "datacheck = data['message3'][106]\n",
    "print(datacheck)\n",
    "bowcheck= bow_transformer.transform([datacheck])\n",
    "print(bowcheck)\n",
    "print(bowcheck.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The above result implies that there are 7 unique words in this particular message (after removing common stopwords). Two of them appear twice, the rest only once. Let's go ahead and check and confirm which ones appear twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks\n",
      "birthday\n"
     ]
    }
   ],
   "source": [
    "print(bow_transformer.get_feature_names()[3953])\n",
    "print(bow_transformer.get_feature_names()[5016])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's perform this on the entire dataset and calculate the sparsity metric of the matrix (a non-negative real number). If sparsity=0, then it's a bad sign. The further away sparsity is from 0, the better because that indicates we have quite a dense dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object type:  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Shape of Sparse Matrix:  (5572, 10943)\n",
      "Amount of Non-Zero occurences:  50458\n",
      "sparsity: 0.08275276724348364\n"
     ]
    }
   ],
   "source": [
    "messages_bow = bow_transformer.transform(data['message3'])\n",
    "print('Object type: ', type(messages_bow))\n",
    "print('Shape of Sparse Matrix: ', messages_bow.shape)\n",
    "print('Amount of Non-Zero occurences: ', messages_bow.nnz)\n",
    "sparsity = (100.0 * messages_bow.nnz / (messages_bow.shape[0] * messages_bow.shape[1])) # nnz stands for non-zero\n",
    "print('sparsity: {}'.format(sparsity)) # this really just tells you how many non-zero entries are there in the entire matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the concepts of TF and IDF in our model and explore them in more details. Recall that the TF measure computes how frequently a term occurs in a document, that is, the number of times a word appears in a document. Sometimes. People define TF in its normalized version (normalized term frequency, a.k.a NTF): NTF(t,d)=number of occurrences of term t in document d/len(d), where len(d) denotes the total number of words in that document. The rationale for the division is that since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length as a way of normalization. The IDF is computed as the logarithm of the total number of the documents in the corpus divided by the number of documents where the specific term appears. It really intends to measure how important a term is. The rationale is that while computing the term frequency, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones. Finally, the TF-IDF weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the TF-IDF weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. In simple terms, TF-IDF=(TF)(IDF). If TF is defined in the normalized version, then TF-IDF=(NTF)(IDF). Let's see an empirical example below below:\n",
    "\n",
    "Consider a document (d) containing 100 words wherein the term (t) 'cat' appears 3 times. The normalized term frequency for cat is then TF=(3/100) = 0.03. Now, assume we have 90 documents (D=90) and the word 'cat' appears in one 1000 of these. Then, the inverse document frequency (IDF) is calculated as IDF=log(D/T(t))=log(90/1000)=log(0.09). \n",
    "\n",
    "Now let's implement these in Python. Below, the TfidfTransformer() function transform a count matrix to a normalized TF or TF-IDF representation. When we use the argument combination (use_idf=False, smooth_idf=False, norm=None), we get a matrix of TF, whereas if we use (use_idf=True, smooth_idf=False, norm=None), we get a matrix of TF-IDF. Let's create two instances and compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 10943)\n",
      "(5572, 10943)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "tf_transformer= TfidfTransformer(use_idf=False, smooth_idf=False, norm=None).fit(messages_bow) # creating an instance and getting the TF vectors\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True, smooth_idf=False, norm=None).fit(messages_bow) # creating an instance and getting the IDF vectors \n",
    "data_tf=tf_transformer.transform(messages_bow) # transforming the entire bag-of-words corpus into NTF corpus at once\n",
    "data_tfidf = tfidf_transformer.transform(messages_bow) # transforming the entire bag-of-words corpus into TF-IDF corpus at once  \n",
    "print(data_tf.shape)\n",
    "print(data_tfidf.shape)\n",
    "print(type(data_tf))\n",
    "print(type(data_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the arguments in the TfidfTransformer() functions are very versatile. In the above example, we are simply getting the matrix for the raw TF and TF-IDF. If the argument smooth_idf is set to be 'True', the constant '1' is added to the numerator and denominator of the IDF as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions. In this case, the IDF is computed as log[(D+1)/(1+T(t))]+1. Most of the machine learning models requires some type of normalization. But here, for simplicity and validation purposes, we will use all metrics with no normalization. This will help us validate our existing numbers and understand the intuition better. More details about these arguments can be found in the online official documentation. Now let's check some examples for validation purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw TF matrix (not normalized) for the 107th message:\n",
      "  (0, 3953)\t2.0\n",
      "  (0, 5016)\t2.0\n",
      "  (0, 7668)\t1.0\n",
      "  (0, 7771)\t1.0\n",
      "  (0, 7867)\t1.0\n",
      "  (0, 10207)\t1.0\n",
      "  (0, 10630)\t1.0\n",
      "TF-IDF matrix for (with no normalization) the 107th message:\n",
      "  (0, 10630)\t5.9119372682\n",
      "  (0, 10207)\t8.01607142247\n",
      "  (0, 7867)\t9.6255093349\n",
      "  (0, 7771)\t6.58098689718\n",
      "  (0, 7668)\t5.77536173319\n",
      "  (0, 5016)\t13.0689337631\n",
      "  (0, 3953)\t11.5507234664\n",
      "IDF for the word 'birthday':  6.53446688154\n",
      "IDF for the word 'birthday':  5.77536173319\n"
     ]
    }
   ],
   "source": [
    "tfcheck = tf_transformer.transform(bowcheck)\n",
    "print('Raw TF matrix (not normalized) for the 107th message:')\n",
    "print(tfcheck)\n",
    "tfidfcheck = tfidf_transformer.transform(bowcheck)\n",
    "print('TF-IDF matrix for (with no normalization) the 107th message:')\n",
    "print(tfidfcheck)\n",
    "print(\"IDF for the word 'birthday': \", tfidf_transformer.idf_[bow_transformer.vocabulary_['birthday']])\n",
    "print(\"IDF for the word 'birthday': \", tfidf_transformer.idf_[bow_transformer.vocabulary_['Thanks']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have obtained a good training dataset called 'data_tfidf'. With messages represented as vectors, we can finally train our spam/ham classifier. Now we can actually use almost any sort of classification algorithms. Here, we will choose the naive Bayes classifier algorithm. The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as TF-IDF may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_detect_model = MultinomialNB().fit(X=data_tfidf, y=data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50458,)\n",
      "(5572, 5)\n"
     ]
    }
   ],
   "source": [
    "print(data_tfidf.data.shape)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try classifying our single random message and checking how we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: ham\n",
      "expected: ham\n"
     ]
    }
   ],
   "source": [
    "print('predicted:', spam_detect_model.predict(tfidfcheck)[0])\n",
    "print('expected:', data.label[106]) # checking how the prediction is doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we want to determine how well our model will do overall on the entire dataset. Here, for illustrative purposes, let's skip the step of splitting the data into test dataset and training dataset. Let's just get the prediction. Keep in mind that in real world problems, we need to do the train-test split first prior to all of the codes above. The split must happen at the beginnning of the code. Here we are just showing the result as an example to demonstrate how the code works and how things tie together eventually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'ham' 'spam' ..., 'ham' 'ham' 'ham']\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       1.00      0.99      1.00      4825\n",
      "       spam       0.96      0.99      0.98       747\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yhat = spam_detect_model.predict(data_tfidf)\n",
    "print(yhat)\n",
    "print(classification_report(data['label'], yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "NLTK is a big topic in Python and because of the complexity of each step during the training and testing phase, so people have built a pipline for this type of project in the 'sklearn' library. We will not elaborate here, but the following command will import that library:\n",
    "\n",
    "    - from sklearn.pipeline import Pipeline\n",
    "    \n",
    "This way, we can directly pass message text data and the pipeline will do our pre-processing for us! We can treat it as a model/estimator API. We will omit the details here but the online documentation certainly have more details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "   - https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/\n",
    "   - https://nlp.stanford.edu/IR-book/html/htmledition/contents-1.html \n",
    "   - http://www.nltk.org/book/\n",
    "   - http://scikit-learn.org/stable/modules/pipeline.html\n",
    "   - http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
